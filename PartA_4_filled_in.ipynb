{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4ed53dd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "c5cb62ac-8e88-43e6-bce9-da20fabf38ff",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "987c7c95a0c7dc71b3d85e154cc3a9be",
     "grade": false,
     "grade_id": "cell-6ebb8bd2f22353d3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Question A4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa81c91",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5c8f824c",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "17d770ae590711dc06f03d150970a3f1",
     "grade": false,
     "grade_id": "cell-e34b0415c38ebac4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this section, we will understand the utility of such a neural network in real world scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98b4da1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "fb9411ad-2324-400e-852e-ff5c0ca716f0",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "287259c58079728b66dae175c6082400",
     "grade": false,
     "grade_id": "cell-4f74b97314b65ea1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "#### Please use the real record data named â€˜record.wavâ€™  as a test sample. Preprocess the data using the provided preprocessing script (data_preprocess.ipynb) and prepare the dataset.\n",
    "Do a model prediction on the sample test dataset and obtain the predicted label using a threshold of 0.5. The model used is the optimized pretrained model using the selected optimal batch size and optimal number of neurons.\n",
    "Find the most important features on the model prediction for the test sample using SHAP. Plot the local feature importance with a force plot and explain your observations.  (Refer to the documentation and these three useful references:\n",
    "https://christophm.github.io/interpretable-ml-book/shap.html#examples-5,\n",
    "https://towardsdatascience.com/deep-learning-model-interpretation-using-shap-a21786e91d16,  \n",
    "https://medium.com/mlearning-ai/shap-force-plots-for-classification-d30be430e195)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996766fc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "981c85ca-9a14-4d0a-b44d-814f02c0f8e1",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "30c3b93836aad148380e15933e7dd786",
     "grade": false,
     "grade_id": "cell-b8a265bf37e3b271",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "1. Firstly, we import relevant libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "48c7450b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "58c50f4f",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8f6af6091e2832c850b00e735d1cff11",
     "grade": false,
     "grade_id": "libraries",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from scipy.io import wavfile as wav\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "from common_utils import set_seed\n",
    "\n",
    "# setting seed\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e0d8f7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "d3444c83",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d796a3a33dd56bd5afb55de45b642449",
     "grade": false,
     "grade_id": "cell-293c9e85ad81d29a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To reduce repeated code, place your\n",
    "network (MLP defined in QA1)\n",
    "torch datasets (CustomDataset defined in QA1)\n",
    "loss function (loss_fn defined in QA1)\n",
    "in a separate file called common_utils.py\n",
    "\n",
    "Import them into this file. You will not be repenalised for any error in QA1 here as the code in QA1 will not be remarked.\n",
    "\n",
    "The following code cell will not be marked.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "eef99838",
   "metadata": {
    "deletable": false,
    "id": "72e8e840",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0c623c0417cb6065d1bbb049f211cf1c",
     "grade": false,
     "grade_id": "cell-29dace0045a28b89",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import json\n",
    "\n",
    "def extract_features(filepath):\n",
    "    \n",
    "    '''\n",
    "    Source: https://github.com/danz1ka19/Music-Emotion-Recognition/blob/master/Feature-Extraction.py\n",
    "    Modified to process a single file\n",
    "\n",
    "        function: extract_features\n",
    "        input: path to mp3 files\n",
    "        output: csv file containing features extracted\n",
    "\n",
    "        This function reads the content in a directory and for each audio file detected\n",
    "        reads the file and extracts relevant features using librosa library for audio\n",
    "        signal processing\n",
    "    '''\n",
    "\n",
    "    feature_set = {}  # Features\n",
    "\n",
    "    # Reading audio file\n",
    "    y, sr = librosa.load(filepath)\n",
    "    S = np.abs(librosa.stft(y, n_fft=512)) \n",
    "    # https://librosa.org/doc/main/generated/librosa.stft.html (set 512 for speech processing)\n",
    "\n",
    "    # Extracting Features\n",
    "    tempo, beats = librosa.beat.beat_track(y=y, sr=sr)\n",
    "    chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr, n_fft=512)\n",
    "    \n",
    "    chroma_cq = librosa.feature.chroma_cqt(y=y, sr=sr)\n",
    "    \n",
    "    chroma_cens = librosa.feature.chroma_cens(y=y, sr=sr)\n",
    "    melspectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=512)\n",
    "    rmse = librosa.feature.rms(y=y)[0]\n",
    "    cent = librosa.feature.spectral_centroid(y=y, sr=sr, n_fft=512)\n",
    "    spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr, n_fft=512)\n",
    "    contrast = librosa.feature.spectral_contrast(S=S, sr=sr, n_fft=512)\n",
    "    rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr, n_fft=512)\n",
    "    poly_features = librosa.feature.poly_features(S=S, sr=sr, n_fft=512)\n",
    "    \n",
    "    tonnetz = librosa.feature.tonnetz(y=y, sr=sr)\n",
    "    \n",
    "    zcr = librosa.feature.zero_crossing_rate(y)\n",
    "    harmonic = librosa.effects.harmonic(y)\n",
    "    percussive = librosa.effects.percussive(y)\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_fft=512)\n",
    "    mfcc_delta = librosa.feature.delta(mfcc)\n",
    "\n",
    "    onset_frames = librosa.onset.onset_detect(y=y, sr=sr)\n",
    "    frames_to_time = librosa.frames_to_time(onset_frames[:20], sr=sr)\n",
    "\n",
    "    # Concatenating Features into one csv and json format\n",
    "    feature_set['filename'] = filepath  # song name\n",
    "    feature_set['tempo'] = tempo  # tempo \n",
    "    feature_set['total_beats'] = sum(beats)  # beats\n",
    "    feature_set['average_beats'] = np.average(beats)\n",
    "    feature_set['chroma_stft_mean'] = np.mean(chroma_stft)  # chroma stft\n",
    "    feature_set['chroma_stft_var'] = np.var(chroma_stft)\n",
    "    \n",
    "    feature_set['chroma_cq_mean'] = np.mean(chroma_cq)  # chroma cq\n",
    "    feature_set['chroma_cq_var'] = np.var(chroma_cq)\n",
    "    \n",
    "    feature_set['chroma_cens_mean'] = np.mean(chroma_cens)  # chroma cens\n",
    "    feature_set['chroma_cens_var'] = np.var(chroma_cens)\n",
    "    feature_set['melspectrogram_mean'] = np.mean(melspectrogram)  # melspectrogram\n",
    "    feature_set['melspectrogram_var'] = np.var(melspectrogram)\n",
    "    feature_set['mfcc_mean'] = np.mean(mfcc)  # mfcc\n",
    "    feature_set['mfcc_var'] = np.var(mfcc)\n",
    "    feature_set['mfcc_delta_mean'] = np.mean(mfcc_delta)  # mfcc delta\n",
    "    feature_set['mfcc_delta_var'] = np.var(mfcc_delta)\n",
    "    feature_set['rmse_mean'] = np.mean(rmse)  # rmse\n",
    "    feature_set['rmse_var'] = np.var(rmse)\n",
    "    feature_set['cent_mean'] = np.mean(cent)  # cent\n",
    "    feature_set['cent_var'] = np.var(cent)\n",
    "    feature_set['spec_bw_mean'] = np.mean(spec_bw)  # spectral bandwidth\n",
    "    feature_set['spec_bw_var'] = np.var(spec_bw)\n",
    "    feature_set['contrast_mean'] = np.mean(contrast)  # contrast\n",
    "    feature_set['contrast_var'] = np.var(contrast)\n",
    "    feature_set['rolloff_mean'] = np.mean(rolloff)  # rolloff\n",
    "    feature_set['rolloff_var'] = np.mean(rolloff)\n",
    "    feature_set['poly_mean'] = np.mean(poly_features)  # poly features\n",
    "    feature_set['poly_var'] = np.mean(poly_features)\n",
    "    \n",
    "    feature_set['tonnetz_mean'] = np.mean(tonnetz)  # tonnetz\n",
    "    feature_set['tonnetz_var'] = np.var(tonnetz)\n",
    "    \n",
    "    feature_set['zcr_mean'] = np.mean(zcr)  # zero crossing rate\n",
    "    feature_set['zcr_var'] = np.var(zcr)\n",
    "    feature_set['harm_mean'] = np.mean(harmonic)  # harmonic\n",
    "    feature_set['harm_var'] = np.var(harmonic)\n",
    "    feature_set['perc_mean'] = np.mean(percussive)  # percussive\n",
    "    feature_set['perc_var'] = np.var(percussive)\n",
    "    feature_set['frame_mean'] = np.mean(frames_to_time)  # frames\n",
    "    feature_set['frame_var'] = np.var(frames_to_time)\n",
    "    \n",
    "    for ix, coeff in enumerate(mfcc):\n",
    "        feature_set['mfcc' + str(ix) + '_mean'] = coeff.mean()\n",
    "        feature_set['mfcc' + str(ix) + '_var'] = coeff.var()\n",
    "    \n",
    "    return feature_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b870a7f4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "b12f3ced-a6a1-4628-a409-1ca7bdfd1cfa",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8dbab024c3394801484199efdbbdb269",
     "grade": true,
     "grade_id": "corrected",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a84ee778",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "18fd5d5e",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7da5539e4fe97549a11c7d61be647167",
     "grade": false,
     "grade_id": "cell-1c5bf554b8f89a3d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "2. Install and import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8b40347e",
   "metadata": {
    "deletable": false,
    "id": "e49be1fc",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f58a0104d88201d0af7de9fc3a6ca035",
     "grade": false,
     "grade_id": "import_shap",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3fd960",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ef497933-2108-4aa5-8ec8-5729214cb1cd",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4cf4df5a01325e8ea1f585dcfc81b01b",
     "grade": true,
     "grade_id": "import_shap_correct",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39337ffe",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "c5fde60a",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b8877105a451813ab23b45e9a180bc36",
     "grade": false,
     "grade_id": "cell-82dd5a271bf5af4b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "3. Read the csv data preprocessed from 'record.wav', using variable name 'df', and fill the size of 'df' in 'size_row' and 'size_column'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f64e869d",
   "metadata": {
    "deletable": false,
    "id": "81a54d47",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c35348846173e5c042d78be10546ae86",
     "grade": false,
     "grade_id": "cell-01d5f7ef70e69e09",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "new_features_dict = extract_features('record.wav')\n",
    "df = pd.DataFrame([new_features_dict])\n",
    "df.to_csv('./new_record.csv', index=False)\n",
    "\n",
    "# df = 0\n",
    "# size_row = 0\n",
    "# size_column = 0\n",
    "# YOUR CODE HERE\n",
    "# Use data_preprocess.ipynb? I think we use data_preprocess.ipynb here to process record.wav for using as test_data.\n",
    "# Here you will use record.wav as test data, the normalization or preprocessing idea will be same with previous tasks. \n",
    "# The preprocess function defined here is for the real data 'df', and it would be better not to modify it. \n",
    "\n",
    "df = pd.read_csv('new_record.csv')\n",
    "size_row, size_column = df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2716e64",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "571b0b06-1750-4228-88af-67d8c52035dc",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d137f7e21ec2ea9ad7a57f4411b513a",
     "grade": true,
     "grade_id": "cell-01d5f7ef70e69e0988",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d953f4d9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "558aa470-6d7e-454c-9cda-9ad881d58c53",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d3d13eea6f0ed0d345e10f33dd3a26da",
     "grade": false,
     "grade_id": "cell-7096e580d10284df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    " 4.  Preprocess to obtain the test data, save the test data as numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "22017114",
   "metadata": {
    "deletable": false,
    "id": "8c77bd18-c546-473e-8c2f-643b4281d9ba",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b19be33055efd5fc5d562a9c671b6eb2",
     "grade": false,
     "grade_id": "cell-b1e392e8ecab207a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            tempo  total_beats  average_beats  chroma_stft_mean  \\\n",
      "5358    95.703125         1874     187.400000          0.567137   \n",
      "642    103.359375          477      79.500000          0.549953   \n",
      "7565    78.302557          875     125.000000          0.646271   \n",
      "9584   112.347147         3430     201.764706          0.599859   \n",
      "9374   198.768029         6870     214.687500          0.724747   \n",
      "...           ...          ...            ...               ...   \n",
      "7813   151.999081         3349     176.263158          0.591543   \n",
      "10955  107.666016         3107     194.187500          0.514742   \n",
      "905    161.499023        16138     375.302326          0.492115   \n",
      "5192    92.285156          247      61.750000          0.526634   \n",
      "235     95.703125          602      86.000000          0.500863   \n",
      "\n",
      "       chroma_stft_var  chroma_cq_mean  chroma_cq_var  chroma_cens_mean  \\\n",
      "5358          0.088985        0.515726       0.076869          0.262738   \n",
      "642           0.088597        0.488051       0.072914          0.261439   \n",
      "7565          0.054740        0.475754       0.066636          0.259040   \n",
      "9584          0.076681        0.494668       0.085328          0.256567   \n",
      "9374          0.043357        0.539541       0.058360          0.263587   \n",
      "...                ...             ...            ...               ...   \n",
      "7813          0.071940        0.528416       0.065232          0.270272   \n",
      "10955         0.092520        0.511646       0.074506          0.271299   \n",
      "905           0.093797        0.469686       0.078947          0.263121   \n",
      "5192          0.099779        0.491312       0.074828          0.265789   \n",
      "235           0.088452        0.395492       0.093293          0.241050   \n",
      "\n",
      "       chroma_cens_var  melspectrogram_mean  ...  mfcc15_var  mfcc16_mean  \\\n",
      "5358          0.014302             0.015899  ...  117.286774     6.089151   \n",
      "642           0.014983             0.018391  ...  111.303917     3.540125   \n",
      "7565          0.016232             0.124626  ...   49.751530     0.434910   \n",
      "9584          0.017507             0.004477  ...   62.477417    -3.350802   \n",
      "9374          0.013855             0.014140  ...   58.752964    -2.410653   \n",
      "...                ...                  ...  ...         ...          ...   \n",
      "7813          0.010286             0.032054  ...  164.046249    -0.369736   \n",
      "10955         0.009730             0.040926  ...  102.433891     6.218508   \n",
      "905           0.014101             0.019817  ...   65.280060    -2.093727   \n",
      "5192          0.012689             0.164569  ...  107.594589    -3.633551   \n",
      "235           0.025228             0.192166  ...  120.908699    -4.513874   \n",
      "\n",
      "       mfcc16_var  mfcc17_mean  mfcc17_var  mfcc18_mean  mfcc18_var  \\\n",
      "5358    84.147858    -2.422905   67.536301    -4.414438   58.760742   \n",
      "642     60.906502     1.795746   82.610077    -2.166018   77.132889   \n",
      "7565    47.341629    -0.417416   65.334297    -1.891326   72.152039   \n",
      "9584    67.107170    -3.099106   70.017227    -3.497207   53.586472   \n",
      "9374    48.343849     5.383557   52.549114    -4.578301   51.342655   \n",
      "...           ...          ...         ...          ...         ...   \n",
      "7813    89.608910     2.543516   68.398201    -5.682937  110.162849   \n",
      "10955   62.830105    -4.612158   68.126984    -0.780457   64.284851   \n",
      "905     60.009510    -1.603222   57.384533    -4.032818   58.510933   \n",
      "5192    59.098770    -4.288532   50.693542    -5.702360   67.433617   \n",
      "235     92.010963    -3.100434   92.593933    -9.613089   92.431259   \n",
      "\n",
      "       mfcc19_mean  mfcc19_var  label  \n",
      "5358      2.859649   57.288010      0  \n",
      "642      -0.994072   82.454002      1  \n",
      "7565      3.552524   44.058418      1  \n",
      "9584     -4.491953   59.188267      0  \n",
      "9374      4.329215   50.392876      0  \n",
      "...            ...         ...    ...  \n",
      "7813     -0.494317   67.964111      1  \n",
      "10955    -2.875442   70.085495      0  \n",
      "905      -0.537293   63.688316      0  \n",
      "5192     -4.019995   47.300236      0  \n",
      "235      -5.749306   81.220169      1  \n",
      "\n",
      "[8439 rows x 78 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X has feature names, but StandardScaler was fitted without feature names\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train_accuracy: 53.18%, Train_loss: 0.691306, Test_accuracy: 53.70%, Test_loss: 0.688969\n",
      "Epoch 2: Train_accuracy: 56.42%, Train_loss: 0.684554, Test_accuracy: 56.58%, Test_loss: 0.682408\n",
      "Epoch 3: Train_accuracy: 57.58%, Train_loss: 0.677616, Test_accuracy: 57.88%, Test_loss: 0.675637\n",
      "Epoch 4: Train_accuracy: 59.54%, Train_loss: 0.667388, Test_accuracy: 59.37%, Test_loss: 0.669969\n",
      "Epoch 5: Train_accuracy: 60.53%, Train_loss: 0.661852, Test_accuracy: 59.51%, Test_loss: 0.668730\n",
      "Epoch 6: Train_accuracy: 61.78%, Train_loss: 0.654298, Test_accuracy: 60.01%, Test_loss: 0.665563\n",
      "Epoch 7: Train_accuracy: 63.56%, Train_loss: 0.644742, Test_accuracy: 61.28%, Test_loss: 0.659559\n",
      "Epoch 8: Train_accuracy: 63.93%, Train_loss: 0.641845, Test_accuracy: 60.92%, Test_loss: 0.656881\n",
      "Epoch 9: Train_accuracy: 65.41%, Train_loss: 0.631541, Test_accuracy: 62.30%, Test_loss: 0.654725\n",
      "Epoch 10: Train_accuracy: 66.23%, Train_loss: 0.625010, Test_accuracy: 61.64%, Test_loss: 0.653578\n",
      "Epoch 11: Train_accuracy: 67.29%, Train_loss: 0.618339, Test_accuracy: 63.46%, Test_loss: 0.640652\n",
      "Epoch 12: Train_accuracy: 68.40%, Train_loss: 0.609530, Test_accuracy: 62.71%, Test_loss: 0.646145\n",
      "Epoch 13: Train_accuracy: 68.34%, Train_loss: 0.603602, Test_accuracy: 62.77%, Test_loss: 0.643162\n",
      "Epoch 14: Train_accuracy: 69.52%, Train_loss: 0.597597, Test_accuracy: 64.95%, Test_loss: 0.632443\n",
      "Epoch 15: Train_accuracy: 70.60%, Train_loss: 0.589871, Test_accuracy: 65.45%, Test_loss: 0.629182\n",
      "Epoch 16: Train_accuracy: 71.56%, Train_loss: 0.584604, Test_accuracy: 64.62%, Test_loss: 0.636773\n",
      "Epoch 17: Train_accuracy: 71.49%, Train_loss: 0.579878, Test_accuracy: 64.68%, Test_loss: 0.636162\n",
      "Epoch 18: Train_accuracy: 72.84%, Train_loss: 0.571733, Test_accuracy: 65.23%, Test_loss: 0.632351\n",
      "Epoch 19: Train_accuracy: 73.21%, Train_loss: 0.566190, Test_accuracy: 64.87%, Test_loss: 0.629896\n",
      "Epoch 20: Train_accuracy: 73.30%, Train_loss: 0.565121, Test_accuracy: 66.39%, Test_loss: 0.625135\n",
      "Epoch 21: Train_accuracy: 74.42%, Train_loss: 0.555758, Test_accuracy: 66.22%, Test_loss: 0.625880\n",
      "Epoch 22: Train_accuracy: 74.01%, Train_loss: 0.557305, Test_accuracy: 66.25%, Test_loss: 0.625670\n",
      "Epoch 23: Train_accuracy: 74.24%, Train_loss: 0.555999, Test_accuracy: 65.75%, Test_loss: 0.623898\n",
      "Epoch 24: Train_accuracy: 75.67%, Train_loss: 0.544302, Test_accuracy: 67.00%, Test_loss: 0.615259\n",
      "Epoch 25: Train_accuracy: 75.73%, Train_loss: 0.544356, Test_accuracy: 66.89%, Test_loss: 0.619982\n",
      "Epoch 26: Train_accuracy: 76.47%, Train_loss: 0.535380, Test_accuracy: 67.25%, Test_loss: 0.614893\n",
      "Epoch 27: Train_accuracy: 76.83%, Train_loss: 0.537392, Test_accuracy: 66.69%, Test_loss: 0.620357\n",
      "Epoch 28: Train_accuracy: 77.20%, Train_loss: 0.529042, Test_accuracy: 67.77%, Test_loss: 0.609699\n",
      "Epoch 29: Train_accuracy: 77.15%, Train_loss: 0.532991, Test_accuracy: 67.88%, Test_loss: 0.610673\n",
      "Epoch 30: Train_accuracy: 77.44%, Train_loss: 0.527567, Test_accuracy: 68.41%, Test_loss: 0.610139\n",
      "Epoch 31: Train_accuracy: 77.69%, Train_loss: 0.525718, Test_accuracy: 69.07%, Test_loss: 0.599071\n",
      "Epoch 32: Train_accuracy: 78.87%, Train_loss: 0.518002, Test_accuracy: 68.46%, Test_loss: 0.605974\n",
      "Epoch 33: Train_accuracy: 79.09%, Train_loss: 0.515998, Test_accuracy: 68.08%, Test_loss: 0.611362\n",
      "Epoch 34: Train_accuracy: 78.73%, Train_loss: 0.519618, Test_accuracy: 67.22%, Test_loss: 0.615092\n",
      "Epoch 35: Train_accuracy: 79.32%, Train_loss: 0.512741, Test_accuracy: 69.10%, Test_loss: 0.602113\n",
      "Epoch 36: Train_accuracy: 79.87%, Train_loss: 0.508304, Test_accuracy: 69.35%, Test_loss: 0.595552\n",
      "Epoch 37: Train_accuracy: 79.17%, Train_loss: 0.513153, Test_accuracy: 69.40%, Test_loss: 0.601924\n",
      "Epoch 38: Train_accuracy: 79.87%, Train_loss: 0.506519, Test_accuracy: 69.15%, Test_loss: 0.599534\n",
      "Epoch 39: Train_accuracy: 80.71%, Train_loss: 0.497320, Test_accuracy: 70.34%, Test_loss: 0.591691\n",
      "Epoch 40: Train_accuracy: 80.73%, Train_loss: 0.496793, Test_accuracy: 69.60%, Test_loss: 0.595186\n",
      "Epoch 41: Train_accuracy: 81.44%, Train_loss: 0.493421, Test_accuracy: 69.65%, Test_loss: 0.597982\n",
      "Epoch 42: Train_accuracy: 81.29%, Train_loss: 0.496151, Test_accuracy: 69.73%, Test_loss: 0.594532\n",
      "Epoch 43: Train_accuracy: 81.85%, Train_loss: 0.487901, Test_accuracy: 69.68%, Test_loss: 0.592178\n",
      "Epoch 44: Train_accuracy: 81.87%, Train_loss: 0.491451, Test_accuracy: 69.68%, Test_loss: 0.597123\n",
      "Epoch 45: Train_accuracy: 82.41%, Train_loss: 0.484349, Test_accuracy: 70.20%, Test_loss: 0.589680\n",
      "Epoch 46: Train_accuracy: 82.34%, Train_loss: 0.480912, Test_accuracy: 70.23%, Test_loss: 0.592687\n",
      "Epoch 47: Train_accuracy: 82.50%, Train_loss: 0.483565, Test_accuracy: 70.78%, Test_loss: 0.584682\n",
      "Epoch 48: Train_accuracy: 83.04%, Train_loss: 0.479585, Test_accuracy: 69.98%, Test_loss: 0.595787\n",
      "Epoch 49: Train_accuracy: 81.79%, Train_loss: 0.486781, Test_accuracy: 69.79%, Test_loss: 0.592173\n",
      "Epoch 50: Train_accuracy: 83.19%, Train_loss: 0.475365, Test_accuracy: 69.40%, Test_loss: 0.598151\n",
      "Epoch 51: Train_accuracy: 83.88%, Train_loss: 0.471835, Test_accuracy: 71.48%, Test_loss: 0.579429\n",
      "Epoch 52: Train_accuracy: 83.52%, Train_loss: 0.474957, Test_accuracy: 71.09%, Test_loss: 0.588302\n",
      "Epoch 53: Train_accuracy: 84.22%, Train_loss: 0.465375, Test_accuracy: 71.45%, Test_loss: 0.576959\n",
      "Epoch 54: Train_accuracy: 84.17%, Train_loss: 0.468981, Test_accuracy: 71.39%, Test_loss: 0.580743\n",
      "Epoch 55: Train_accuracy: 84.01%, Train_loss: 0.467039, Test_accuracy: 70.87%, Test_loss: 0.588761\n",
      "Epoch 56: Train_accuracy: 84.07%, Train_loss: 0.465184, Test_accuracy: 69.90%, Test_loss: 0.589237\n",
      "Epoch 57: Train_accuracy: 84.31%, Train_loss: 0.462664, Test_accuracy: 71.56%, Test_loss: 0.580670\n",
      "Epoch 58: Train_accuracy: 84.89%, Train_loss: 0.461162, Test_accuracy: 72.17%, Test_loss: 0.573939\n",
      "Epoch 59: Train_accuracy: 85.34%, Train_loss: 0.459694, Test_accuracy: 72.22%, Test_loss: 0.576458\n",
      "Epoch 60: Train_accuracy: 85.37%, Train_loss: 0.459383, Test_accuracy: 71.01%, Test_loss: 0.583069\n",
      "Epoch 61: Train_accuracy: 84.69%, Train_loss: 0.460958, Test_accuracy: 71.50%, Test_loss: 0.579497\n",
      "Epoch 62: Train_accuracy: 85.44%, Train_loss: 0.454558, Test_accuracy: 72.03%, Test_loss: 0.575302\n",
      "Epoch 63: Train_accuracy: 85.37%, Train_loss: 0.455380, Test_accuracy: 71.61%, Test_loss: 0.576196\n",
      "Epoch 64: Train_accuracy: 85.39%, Train_loss: 0.454761, Test_accuracy: 71.14%, Test_loss: 0.584982\n",
      "Epoch 65: Train_accuracy: 85.37%, Train_loss: 0.454146, Test_accuracy: 71.25%, Test_loss: 0.582648\n",
      "Epoch 66: Train_accuracy: 85.26%, Train_loss: 0.455524, Test_accuracy: 71.59%, Test_loss: 0.581796\n",
      "Epoch 67: Train_accuracy: 86.09%, Train_loss: 0.448258, Test_accuracy: 71.56%, Test_loss: 0.574333\n",
      "Epoch 68: Train_accuracy: 86.38%, Train_loss: 0.447414, Test_accuracy: 72.91%, Test_loss: 0.570757\n",
      "Epoch 69: Train_accuracy: 86.04%, Train_loss: 0.450838, Test_accuracy: 73.22%, Test_loss: 0.565110\n",
      "Epoch 70: Train_accuracy: 86.43%, Train_loss: 0.447436, Test_accuracy: 72.31%, Test_loss: 0.575076\n",
      "Epoch 71: Train_accuracy: 86.04%, Train_loss: 0.448477, Test_accuracy: 72.86%, Test_loss: 0.571991\n",
      "Epoch 72: Train_accuracy: 86.79%, Train_loss: 0.443392, Test_accuracy: 72.89%, Test_loss: 0.573676\n",
      "Epoch 73: Train_accuracy: 86.29%, Train_loss: 0.443649, Test_accuracy: 72.47%, Test_loss: 0.570237\n",
      "Epoch 74: Train_accuracy: 86.69%, Train_loss: 0.441712, Test_accuracy: 72.75%, Test_loss: 0.570949\n",
      "Epoch 75: Train_accuracy: 86.65%, Train_loss: 0.442617, Test_accuracy: 72.03%, Test_loss: 0.578234\n",
      "Epoch 76: Train_accuracy: 87.61%, Train_loss: 0.434620, Test_accuracy: 72.78%, Test_loss: 0.569524\n",
      "Epoch 77: Train_accuracy: 87.56%, Train_loss: 0.435218, Test_accuracy: 73.22%, Test_loss: 0.566110\n",
      "Epoch 78: Train_accuracy: 87.11%, Train_loss: 0.439018, Test_accuracy: 72.69%, Test_loss: 0.571657\n",
      "Epoch 79: Train_accuracy: 87.07%, Train_loss: 0.440103, Test_accuracy: 72.42%, Test_loss: 0.573066\n",
      "Epoch 80: Train_accuracy: 87.68%, Train_loss: 0.431794, Test_accuracy: 72.83%, Test_loss: 0.569152\n",
      "Epoch 81: Train_accuracy: 87.97%, Train_loss: 0.429985, Test_accuracy: 72.58%, Test_loss: 0.571743\n",
      "Epoch 82: Train_accuracy: 88.13%, Train_loss: 0.430317, Test_accuracy: 73.11%, Test_loss: 0.566351\n",
      "Epoch 83: Train_accuracy: 87.88%, Train_loss: 0.432778, Test_accuracy: 72.83%, Test_loss: 0.567752\n",
      "Epoch 84: Train_accuracy: 88.26%, Train_loss: 0.430203, Test_accuracy: 73.80%, Test_loss: 0.562932\n",
      "Epoch 85: Train_accuracy: 87.46%, Train_loss: 0.435727, Test_accuracy: 72.50%, Test_loss: 0.573997\n",
      "Epoch 86: Train_accuracy: 88.13%, Train_loss: 0.429286, Test_accuracy: 73.58%, Test_loss: 0.563137\n",
      "Epoch 87: Train_accuracy: 87.61%, Train_loss: 0.431710, Test_accuracy: 72.89%, Test_loss: 0.567978\n",
      "Epoch 88: Train_accuracy: 88.26%, Train_loss: 0.430175, Test_accuracy: 73.30%, Test_loss: 0.569116\n",
      "Epoch 89: Train_accuracy: 87.55%, Train_loss: 0.434245, Test_accuracy: 72.66%, Test_loss: 0.568196\n",
      "Epoch 90: Train_accuracy: 88.06%, Train_loss: 0.425702, Test_accuracy: 73.36%, Test_loss: 0.569940\n",
      "Epoch 91: Train_accuracy: 89.20%, Train_loss: 0.418103, Test_accuracy: 73.00%, Test_loss: 0.565902\n",
      "Epoch 92: Train_accuracy: 88.64%, Train_loss: 0.427590, Test_accuracy: 73.22%, Test_loss: 0.567256\n",
      "Epoch 93: Train_accuracy: 89.75%, Train_loss: 0.416203, Test_accuracy: 72.72%, Test_loss: 0.569423\n",
      "Epoch 94: Train_accuracy: 88.80%, Train_loss: 0.425073, Test_accuracy: 73.05%, Test_loss: 0.567831\n",
      "Epoch 95: Train_accuracy: 89.00%, Train_loss: 0.420836, Test_accuracy: 73.74%, Test_loss: 0.564719\n",
      "Epoch 96: Train_accuracy: 88.62%, Train_loss: 0.426719, Test_accuracy: 73.36%, Test_loss: 0.568173\n",
      "Epoch 97: Train_accuracy: 89.35%, Train_loss: 0.415127, Test_accuracy: 73.91%, Test_loss: 0.563305\n",
      "Epoch 98: Train_accuracy: 88.64%, Train_loss: 0.423334, Test_accuracy: 73.33%, Test_loss: 0.570327\n",
      "Epoch 99: Train_accuracy: 89.29%, Train_loss: 0.421258, Test_accuracy: 73.58%, Test_loss: 0.565940\n",
      "Epoch 100: Train_accuracy: 88.88%, Train_loss: 0.420522, Test_accuracy: 72.72%, Test_loss: 0.570410\n"
     ]
    }
   ],
   "source": [
    "# Are we allowed to save and load the pretrained model from A3? Otherwise, for retraining the model I believe the preprocess needs to return the scaled X_train and scaled X-test. \n",
    "# Yes, you are allowed to save the pre-trained model from A3.\n",
    "# You can use the pre-trained model from the last step in Part A3. You can either save the pre-trained model in PartA 3, or retrain and save it using the similar data split with PartA 3 in the code cells of PartA 4. Then you can proceed with the prediction of 'record.wav'.\n",
    "# so we do five folds cv, and use the best model from the 5 folds?, or do we just do train test split, and train it once and use that model?\n",
    "# You just do train test split, and train it once and then use that model, no need k-fold cv.\n",
    "\n",
    "# Basically, what I think it means is that you have already done the validation in A3 using the whole simplified.csv and the training in A3 so you can just use the model in A3 to do the prediction on record.wav.\n",
    "\n",
    "# No, you need to use the optimal number of neurons found in A3 only for the first layer of the model in A4. The number of neurons for other layers can be set the same with previous tasks.\n",
    "from common_utils import preprocess_dataset, split_dataset\n",
    "# preprocess_dataset scales both df_train and df_test\n",
    "\n",
    "# print(size_row)\n",
    "# print(size_column)\n",
    "# print(df)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# def preprocess(df):\n",
    "#     # YOUR CODE HERE\n",
    "    \n",
    "#     X_train, y_train, X_test, y_test = split_dataset(df, 'filename', 0.30, 1)\n",
    "#     X_train_scaled, X_test_scaled = preprocess_dataset(X_train, X_test)\n",
    "    \n",
    "#     return X_train_scaled, y_train, X_test_scaled, y_test\n",
    "\n",
    "# X_train, y_train, X_test, y_test = preprocess(df)\n",
    "# X_train and y_train go into intialise loaders batch\n",
    "\n",
    "# def intialise_loaders_batch(X_train_scaled, y_train, X_test_scaled, y_test, batch_size):\n",
    "\n",
    "# #     print(\"X_train_scaled in initialise loaders batch\")\n",
    "# #     print(len(X_train_scaled[0]))\n",
    "#     train_data = BatchCustomDataset(X_train_scaled,y_train)\n",
    "# #     print(len(train_data[1]))\n",
    "#     test_data = BatchCustomDataset(X_test_scaled,y_test)\n",
    "    \n",
    "#     train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "#     test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "#     return train_dataloader, test_dataloader\n",
    "\n",
    "# just bring out the train_data and throw into preprocess\n",
    "\n",
    "def preprocess(X_train, df):\n",
    "    \"\"\"preprocess your dataset to obtain your test dataset, remember to remove the 'filename' as Q1\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    df = df.drop(columns=['filename'])\n",
    "    X_train_scaled, X_test_scaled_eg = preprocess_dataset(X_train, df)\n",
    "\n",
    "    # note: already a numpy array\n",
    "    # X_test_scaled_eg is the input features into our model later\n",
    "    return X_test_scaled_eg\n",
    "\n",
    "# for training the model\n",
    "simplified = pd.read_csv('simplified.csv')\n",
    "simplified['label'] = simplified['filename'].str.split('_').str[-2]\n",
    "simplified['label'].value_counts()\n",
    "\n",
    "X_train_simplified, y_train_simplified, X_test_simplified, y_test_simplified = split_dataset(simplified, 'filename', 0.30, 1)\n",
    "print(X_train_simplified)\n",
    "X_train_simplified = X_train_simplified.drop(columns=['label'])\n",
    "X_test_simplified = X_test_simplified.drop(columns=['label'])\n",
    "X_train_scaled_simplified, X_test_scaled_simplified = preprocess_dataset(X_train_simplified, X_test_simplified)\n",
    "# for training the model\n",
    "\n",
    "X_test_scaled_eg = preprocess(X_train_scaled_simplified, df)\n",
    "\n",
    "# print(df.drop(columns=['filename']))\n",
    "# print(len(X_test_scaled_eg[0]))\n",
    "# print(X_test_scaled_eg)\n",
    "# print(len(X_train_scaled_simplified[0]))\n",
    "# print(X_train_scaled_simplified)\n",
    "\n",
    "# X_test_scaled_eg is to be used for testing\n",
    "\n",
    "# print(X_test_scaled_eg)\n",
    "\n",
    "def train(model, X_train_scaled, y_train2, X_val_scaled, y_val2, batch_size):\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    epochs = 100\n",
    "    times = []\n",
    "    train_dataloader, test_dataloader = intialise_loaders(X_train_scaled, y_train2, X_val_scaled, y_val2)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    tr_loss, tr_correct = [], []\n",
    "    te_loss, te_correct = [], []\n",
    "    for t in range(epochs):\n",
    "        train_loss, train_correct = train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "        test_loss, test_correct = test_loop(test_dataloader, model, loss_fn)\n",
    "\n",
    "        tr_loss.append(train_loss), tr_correct.append(train_correct)\n",
    "        te_loss.append(test_loss), te_correct.append(test_correct)\n",
    "        times.append(t+1)\n",
    "        \n",
    "        print(f\"Epoch {t+1}: Train_accuracy: {(100*train_correct):>0.2f}%, Train_loss: {train_loss:>8f}, Test_accuracy: {(100*test_correct):>0.2f}%, Test_loss: {test_loss:>8f}\")\n",
    "        \n",
    "    train_accuracies = tr_correct\n",
    "    train_losses = tr_loss\n",
    "    test_accuracies = te_correct\n",
    "    test_losses = te_loss\n",
    "\n",
    "    return train_accuracies, train_losses, test_accuracies, test_losses, times\n",
    "\n",
    "def intialise_loaders(X_train_scaled, y_train, X_test_scaled, y_test):\n",
    "    # YOUR CODE HERE\n",
    "#     train_dataset = CustomDataset(X_train_scaled)\n",
    "#     test_dataset = CustomDataset(X_test_scaled)\n",
    "    \n",
    "#     train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "#     test_dataloader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "    train_data = CustomDataset(X_train_scaled,y_train)\n",
    "    test_data = CustomDataset(X_test_scaled,y_test)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_data, batch_size=1024, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=1024, shuffle=True)\n",
    "    \n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    train_loss, train_correct = 0, 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    \n",
    "    train_loss /= num_batches\n",
    "    train_correct /=size\n",
    "\n",
    "    return train_loss, train_correct\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, test_correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            test_correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    test_correct /= size\n",
    "    \n",
    "    return test_loss, test_correct\n",
    "\n",
    "class FirstHiddenLayerMLP(nn.Module):\n",
    "\n",
    "    def __init__(self, no_features, no_hidden_first_layer, no_labels):\n",
    "        super().__init__()\n",
    "        self.mlp_stack = nn.Sequential(\n",
    "            # YOUR CODE HERE\n",
    "            nn.Linear(no_features, no_hidden_first_layer),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(no_hidden_first_layer, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(128, no_labels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    def forward(self, x):\n",
    "         logits = self.mlp_stack(x)\n",
    "         return logits\n",
    "\n",
    "model = FirstHiddenLayerMLP(77,256,2)\n",
    "train_accuracies, train_losses, test_accuracies, test_losses, times = train(model, X_train_scaled_simplified, y_train_simplified, X_test_scaled_simplified, y_test_simplified, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418482bd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "e49ee8a7-d9b2-499d-8394-d6cb86f4cb60",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b0c0b2a92c7d501f1ac652e11f948461",
     "grade": true,
     "grade_id": "cell-fbe8ba077fb74598",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91f8f29a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "b6756ab6-92e0-4a5e-b4b9-aebe009f5480",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "96d2e019d65c49ba15b3089c2184f021",
     "grade": false,
     "grade_id": "cell-48b4edbfec330f39",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "5. Do a model prediction on the sample test dataset and obtain the predicted label using a threshold of 0.5. The model used is the optimized pretrained model using the selected optimal batch size and optimal number of neurons. Note: Please define the variable of your final predicted label as 'pred_label'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2fbc4115",
   "metadata": {
    "deletable": false,
    "id": "8fa3afdf-eed6-47b9-9acc-bc2304c46ec3",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "276ec9575db4ca701823a459809ea810",
     "grade": true,
     "grade_id": "cell-e83cb49660edc2b7",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n",
      "0\n",
      "[[1 0]]\n",
      "[[1 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# Do model prediction on record.wav.\n",
    "# remember to set the 0.5 threshold.\n",
    "\n",
    "print(len(X_test_scaled_eg[0]))\n",
    "X_test_scaled_eg = torch.tensor(X_test_scaled_eg, dtype=torch.float32)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test_scaled_eg)\n",
    "threshold = 0.5\n",
    "predicted_labels = (predictions > threshold).type(torch.long)\n",
    "predicted_labels_array = predicted_labels.numpy()\n",
    "predicted_label = torch.max(predictions, 1)\n",
    "predicted_label_array = predicted_labels.numpy()\n",
    "pred_label = np.argmax(predicted_label_array)\n",
    "print(pred_label)\n",
    "print(predicted_labels_array)\n",
    "print(predicted_label_array)\n",
    "# one hot vector classifies as class on the zeroth column\n",
    "# negative is 0 and positive is 1\n",
    "# so it has classified the record.wav as negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667d83dd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "da2fc2cc-b89f-4fc3-af16-e30b4e5315a3",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "704df2be8fbd85ba163a89cd2e0431f0",
     "grade": true,
     "grade_id": "predict_value",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e45280ce",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "baab6e4d-4e8b-4358-a68d-682f60db4a06",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eac3438866b5ebd40f5fb20a676059bd",
     "grade": false,
     "grade_id": "cell-896f18b6b0b948ea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "6. Find the most important features on the model prediction for your test sample using SHAP. Create an instance of the DeepSHAP which is called DeepExplainer using traianing dataset: https://shap-lrjball.readthedocs.io/en/latest/generated/shap.DeepExplainer.html.\n",
    "\n",
    "Plot the local feature importance with a force plot and explain your observations.  (Refer to the documentation and these three useful references:\n",
    "https://christophm.github.io/interpretable-ml-book/shap.html#examples-5,\n",
    "https://towardsdatascience.com/deep-learning-model-interpretation-using-shap-a21786e91d16,  \n",
    "https://medium.com/mlearning-ai/shap-force-plots-for-classification-d30be430e195)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c3872830",
   "metadata": {
    "deletable": false,
    "id": "081aa567-cd92-4749-93fd-fc6608a1f6ae",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db46b1b26fd45359768421987104ac3e",
     "grade": true,
     "grade_id": "importance_weight",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[159], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m X_train_scaled_simplified \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X_train_scaled_simplified, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[0;32m     12\u001b[0m explainer \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mDeepExplainer(model, X_train_scaled_simplified)\n\u001b[1;32m---> 13\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled_simplified\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m shap\u001b[38;5;241m.\u001b[39minitjs()\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# assuming I expect \u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\gareth thong\\neural networks and deep learning a4\\myenv\\lib\\site-packages\\shap\\explainers\\_deep\\__init__.py:124\u001b[0m, in \u001b[0;36mDeep.shap_values\u001b[1;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshap_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, ranked_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, output_rank_order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m, check_additivity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     91\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Return approximate SHAP values for the model applied to the data given by X.\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \n\u001b[0;32m     93\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;124;03m        were chosen as \"top\".\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mranked_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_rank_order\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_additivity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_additivity\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\gareth thong\\neural networks and deep learning a4\\myenv\\lib\\site-packages\\shap\\explainers\\_deep\\deep_pytorch.py:188\u001b[0m, in \u001b[0;36mPyTorchDeep.shap_values\u001b[1;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# run attribution computation graph\u001b[39;00m\n\u001b[0;32m    187\u001b[0m feature_ind \u001b[38;5;241m=\u001b[39m model_output_ranks[j, i]\n\u001b[1;32m--> 188\u001b[0m sample_phis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_ind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoint_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# assign the attributions to the right part of the output arrays\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterim:\n",
      "File \u001b[1;32mc:\\users\\gareth thong\\neural networks and deep learning a4\\myenv\\lib\\site-packages\\shap\\explainers\\_deep\\deep_pytorch.py:124\u001b[0m, in \u001b[0;36mPyTorchDeep.gradient\u001b[1;34m(self, idx, inputs)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(X):\n\u001b[1;32m--> 124\u001b[0m         grad \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselected\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    127\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    128\u001b[0m             grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\users\\gareth thong\\neural networks and deep learning a4\\myenv\\lib\\site-packages\\torch\\autograd\\__init__.py:394\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[0;32m    390\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[0;32m    391\u001b[0m         grad_outputs_\n\u001b[0;32m    392\u001b[0m     )\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 394\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[0;32m    404\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m    405\u001b[0m         output\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    407\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mzeros_like(\u001b[38;5;28minput\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    408\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m (output, \u001b[38;5;28minput\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(result, t_inputs)\n\u001b[0;32m    409\u001b[0m     )\n",
      "File \u001b[1;32mc:\\users\\gareth thong\\neural networks and deep learning a4\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:64\u001b[0m, in \u001b[0;36m_WrappedHook.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule: weakref\u001b[38;5;241m.\u001b[39mReferenceType[Module] \u001b[38;5;241m=\u001b[39m weakref\u001b[38;5;241m.\u001b[39mref(module)\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_module:\n\u001b[0;32m     66\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "Fit the explainer on a subset of the data (you can try all but then gets slower)\n",
    "Return approximate SHAP values for the model applied to the data given by X.\n",
    "Plot the local feature importance with a force plot and explain your observations.\n",
    "'''\n",
    "# YOUR CODE HERE\n",
    "# Can try shap.force_plot(..., matplotlib=True)\n",
    "# This way your force plot is presented as a matplotlib image rather than an interactive element in your ipynb.\n",
    "\n",
    "\n",
    "X_train_scaled_simplified = torch.tensor(X_train_scaled_simplified, dtype=torch.float)\n",
    "explainer = shap.DeepExplainer(model, X_train_scaled_simplified)\n",
    "shap_values = explainer.shap_values(X_train_scaled_simplified)\n",
    "shap.initjs()\n",
    "# assuming I expect a negative sentiment classification\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[0][76])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
