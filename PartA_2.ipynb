{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ec3f03c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "c5cb62ac-8e88-43e6-bce9-da20fabf38ff",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c7e82aadc4d77a8b23f7f880449f9e3",
     "grade": false,
     "grade_id": "a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Question A2 (10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f2c293",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "26b4ac2a-d56e-4151-8e0a-4a833cbc643e",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eb28aa752ce5540f5b18d10694b52ea9",
     "grade": false,
     "grade_id": "a22",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### In this question, we will determine the optimal batch size for mini-batch gradient descent. Find the optimal batch size for mini-batch gradient descent by training the neural network and evaluating the performances for different batch sizes. Note: Use 5-fold cross-validation on training partition to perform hyperparameter selection. You will have to reconsider the scaling of the dataset during the 5-fold cross validation.\n",
    "\n",
    "* note: some cells are non-editable and cannot be filled, but leave them untouched. Fill up only cells which are provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f67db22",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "fb9411ad-2324-400e-852e-ff5c0ca716f0",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aceec82011f43733c0551ca196f1b16c",
     "grade": false,
     "grade_id": "a2_1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "#### Plot mean cross-validation accuracies on the final epoch for different batch sizes as a scatter plot. Limit search space to batch sizes {128, 256, 512, 1024}. Next, create a table of time taken to train the network on the last epoch against different batch sizes. Finally, select the optimal batch size and state a reason for your selection.\n",
    "\n",
    "This might take a while to run, so plan your time carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ebc88377",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "b0edc610-21e6-4cc7-9603-59318b961990",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "909acb3c7ff3883eb5381eb586615d3b",
     "grade": false,
     "grade_id": "libraries",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from scipy.io import wavfile as wav\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "from common_utils import set_seed\n",
    "\n",
    "# setting seed\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2798997b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "e8e12861-4713-4914-9f4b-8a7381708243",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed97d9f30da032a5e349047c614efec1",
     "grade": false,
     "grade_id": "a2_1_2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "2. To reduce repeated code, place your\n",
    "\n",
    "- network (MLP defined in QA1)\n",
    "- torch datasets (CustomDataset defined in QA1)\n",
    "- loss function (loss_fn defined in QA1)\n",
    "\n",
    "in a separate file called **common_utils.py**\n",
    "\n",
    "Import them into this file. You will not be repenalised for any error in QA1 here as the code in QA1 will not be remarked.\n",
    "\n",
    "The following code cell will not be marked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "de7a1777",
   "metadata": {
    "deletable": false,
    "id": "37a1a982-de85-46de-b890-3b81f79f5887",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9db3ca972642b1447dba3ebd5f2db24b",
     "grade": false,
     "grade_id": "import",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            tempo  total_beats  average_beats  chroma_stft_mean  \\\n",
      "5358    95.703125         1874     187.400000          0.567137   \n",
      "642    103.359375          477      79.500000          0.549953   \n",
      "7565    78.302557          875     125.000000          0.646271   \n",
      "9584   112.347147         3430     201.764706          0.599859   \n",
      "9374   198.768029         6870     214.687500          0.724747   \n",
      "...           ...          ...            ...               ...   \n",
      "7813   151.999081         3349     176.263158          0.591543   \n",
      "10955  107.666016         3107     194.187500          0.514742   \n",
      "905    161.499023        16138     375.302326          0.492115   \n",
      "5192    92.285156          247      61.750000          0.526634   \n",
      "235     95.703125          602      86.000000          0.500863   \n",
      "\n",
      "       chroma_stft_var  chroma_cq_mean  chroma_cq_var  chroma_cens_mean  \\\n",
      "5358          0.088985        0.515726       0.076869          0.262738   \n",
      "642           0.088597        0.488051       0.072914          0.261439   \n",
      "7565          0.054740        0.475754       0.066636          0.259040   \n",
      "9584          0.076681        0.494668       0.085328          0.256567   \n",
      "9374          0.043357        0.539541       0.058360          0.263587   \n",
      "...                ...             ...            ...               ...   \n",
      "7813          0.071940        0.528416       0.065232          0.270272   \n",
      "10955         0.092520        0.511646       0.074506          0.271299   \n",
      "905           0.093797        0.469686       0.078947          0.263121   \n",
      "5192          0.099779        0.491312       0.074828          0.265789   \n",
      "235           0.088452        0.395492       0.093293          0.241050   \n",
      "\n",
      "       chroma_cens_var  melspectrogram_mean  ...  mfcc15_var  mfcc16_mean  \\\n",
      "5358          0.014302             0.015899  ...  117.286774     6.089151   \n",
      "642           0.014983             0.018391  ...  111.303917     3.540125   \n",
      "7565          0.016232             0.124626  ...   49.751530     0.434910   \n",
      "9584          0.017507             0.004477  ...   62.477417    -3.350802   \n",
      "9374          0.013855             0.014140  ...   58.752964    -2.410653   \n",
      "...                ...                  ...  ...         ...          ...   \n",
      "7813          0.010286             0.032054  ...  164.046249    -0.369736   \n",
      "10955         0.009730             0.040926  ...  102.433891     6.218508   \n",
      "905           0.014101             0.019817  ...   65.280060    -2.093727   \n",
      "5192          0.012689             0.164569  ...  107.594589    -3.633551   \n",
      "235           0.025228             0.192166  ...  120.908699    -4.513874   \n",
      "\n",
      "       mfcc16_var  mfcc17_mean  mfcc17_var  mfcc18_mean  mfcc18_var  \\\n",
      "5358    84.147858    -2.422905   67.536301    -4.414438   58.760742   \n",
      "642     60.906502     1.795746   82.610077    -2.166018   77.132889   \n",
      "7565    47.341629    -0.417416   65.334297    -1.891326   72.152039   \n",
      "9584    67.107170    -3.099106   70.017227    -3.497207   53.586472   \n",
      "9374    48.343849     5.383557   52.549114    -4.578301   51.342655   \n",
      "...           ...          ...         ...          ...         ...   \n",
      "7813    89.608910     2.543516   68.398201    -5.682937  110.162849   \n",
      "10955   62.830105    -4.612158   68.126984    -0.780457   64.284851   \n",
      "905     60.009510    -1.603222   57.384533    -4.032818   58.510933   \n",
      "5192    59.098770    -4.288532   50.693542    -5.702360   67.433617   \n",
      "235     92.010963    -3.100434   92.593933    -9.613089   92.431259   \n",
      "\n",
      "       mfcc19_mean  mfcc19_var  label  \n",
      "5358      2.859649   57.288010      0  \n",
      "642      -0.994072   82.454002      1  \n",
      "7565      3.552524   44.058418      1  \n",
      "9584     -4.491953   59.188267      0  \n",
      "9374      4.329215   50.392876      0  \n",
      "...            ...         ...    ...  \n",
      "7813     -0.494317   67.964111      1  \n",
      "10955    -2.875442   70.085495      0  \n",
      "905      -0.537293   63.688316      0  \n",
      "5192     -4.019995   47.300236      0  \n",
      "235      -5.749306   81.220169      1  \n",
      "\n",
      "[8439 rows x 78 columns]\n",
      "[0 1 1 ... 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "from common_utils import MLP, CustomDataset, loss_fn, split_dataset\n",
    "# from common_utils import split_dataset, preprocess_dataset\n",
    "\n",
    "# def preprocess(df):\n",
    "#     # YOUR CODE HERE\n",
    "    \n",
    "#     X_train, y_train, X_test, y_test = split_dataset(df, 'filename', 0.30, 1)\n",
    "#     X_train_scaled, X_test_scaled = preprocess_dataset(X_train, X_test)\n",
    "    \n",
    "#     return X_train_scaled, y_train, X_test_scaled, y_test\n",
    "\n",
    "# import the x train and y train datasets\n",
    "df = pd.read_csv('simplified.csv')\n",
    "df['label'] = df['filename'].str.split('_').str[-2]\n",
    "\n",
    "df['label'].value_counts()\n",
    "#change to train test split?\n",
    "X_train, y_train, X_test, y_test = split_dataset(df, 'filename', 0.30, 1)\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "\n",
    "class BatchCustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# X_train_scaled, y_train, X_test_scaled, y_test = preprocess(df)\n",
    "# X_train, y_train = X_train_scaled, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afee2474",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5aa562e7-23c3-4920-ae63-4563bf30e39d",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae6b33318200b4bc38d431576963edb1",
     "grade": true,
     "grade_id": "correct_import",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f4303a5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "82ea67d6-1eb4-428d-9407-9d988e927ff6",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c738d3b4888de90dda8c532036bc5fe5",
     "grade": false,
     "grade_id": "a2_1_3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "3. Define different folds for different batch sizes to get a dictionary of training and validation datasets. Preprocess your datasets accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7c4c1053",
   "metadata": {
    "deletable": false,
    "id": "deab683a-2c9e-4e62-823a-e8b4a186bda8",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d02dac62baa528c191eb4f47b2495406",
     "grade": false,
     "grade_id": "dataset",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "5\n",
      "6751\n",
      "77\n",
      "[[-0.49106531  0.01265512  0.05686221 ...  1.18119068 -0.38849754\n",
      "  -1.03574435]\n",
      " [-0.79797693 -0.98345107 -0.22072241 ...  0.05499416  0.55841098\n",
      "   0.96548922]\n",
      " [-0.71053897 -0.56340629  1.33516163 ...  1.38367374 -0.88628092\n",
      "   0.96548922]\n",
      " ...\n",
      " [ 2.64264068  1.74732306 -1.15502705 ...  0.18848147 -0.14767633\n",
      "  -1.03574435]\n",
      " [-0.8485064  -1.1473147  -0.59740884 ... -0.8292896  -0.76430263\n",
      "  -1.03574435]\n",
      " [-0.77051526 -0.92344468 -1.01371018 ... -1.33465669  0.51198613\n",
      "   0.96548922]]\n",
      "4\n",
      "5\n",
      "1688\n",
      "77\n",
      "[[-0.82343749 -1.10706956 -0.41688652 ... -0.16468361 -0.68326526\n",
      "  -1.02763283]\n",
      " [ 0.98136812  2.11967717  0.5703326  ...  1.19125302 -0.5304174\n",
      "  -1.02763283]\n",
      " [ 2.26269824  1.56816612 -0.46104367 ... -0.4657886  -0.76811365\n",
      "  -1.02763283]\n",
      " ...\n",
      " [-0.84217921 -1.13612265 -1.27788254 ... -0.73719861  0.23449046\n",
      "   0.97311021]\n",
      " [-0.82817678 -1.14106785  0.41268208 ...  0.68536118 -0.9647467\n",
      "  -1.02763283]\n",
      " [-0.6952614  -0.71454386 -0.90409557 ...  0.72641355 -0.53359531\n",
      "   0.97311021]]\n",
      "4\n",
      "5\n",
      "6751\n",
      "[0 1 1 ... 0 0 1]\n",
      "1\n",
      "4\n",
      "5\n",
      "1688\n",
      "[0 0 0 ... 1 0 1]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "def generate_cv_folds_for_batch_sizes(parameters, X_train, y_train):\n",
    "    \"\"\"\n",
    "    returns:\n",
    "    X_train_scaled_dict(dict) where X_train_scaled_dict[batch_size] is a list of the preprocessed training matrix for the different folds.\n",
    "    X_val_scaled_dict(dict) where X_val_scaled_dict[batch_size] is a list of the processed validation matrix for the different folds.\n",
    "    y_train_dict(dict) where y_train_dict[batch_size] is a list of labels for the different folds\n",
    "    y_val_dict(dict) where y_val_dict[batch_size] is a list of labels for the different folds\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "#     x train scaled and y train goes in here\n",
    "#     x train scaled dict is the list of of 4/5 matrices\n",
    "#     x val scaled dict is the last 1/5 matrix for testing\n",
    "#     y train dict is the list of 4/5 labels\n",
    "#     y val dict is the last 1/5 labels for testing\n",
    "    \n",
    "#     It will not differ by batch size. X_train_scaled_dict[128] is a list of train dataset for the different folds, and you should have 5 elements in the list in total. It is the same as X_train_scaled_dict[256], etc\n",
    "#     X_train_scaled_dict should look like {128:[list of 5 folds] 256:[list of 5 folds], 512: [list of 5 folds], 1024: [list of 5 folds]}\n",
    "#     y_train_dict is a dictionary of 4x5 elements as well, each element is the matrix of labels to train towards\n",
    "    \n",
    "#     customdataset = xtrain, ytrain\n",
    "    \n",
    "#     cv = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "#     for train_idx, test_idx in cv.split(X_train, y_train):\n",
    "#         X_train_scaled_dict, y_train_dict  = X_train[train_idx], y_train[train_idx]\n",
    "#         X_val_scaled_dict, y__val_dict = X_train[test_idx], y_train[test_idx]\n",
    "    batch_sizes = parameters  # Default to batch size of 32 if not provided\n",
    "    \n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    \n",
    "#     X_train_scaled_dict = {batch_size: [] for batch_size in batch_sizes}\n",
    "#     X_val_scaled_dict = {batch_size: [] for batch_size in batch_sizes}\n",
    "#     y_train_dict = {batch_size: [] for batch_size in batch_sizes}\n",
    "#     y_val_dict = {batch_size: [] for batch_size in batch_sizes}\n",
    "\n",
    "    X_train_scaled_dict = {}\n",
    "    X_val_scaled_dict = {}\n",
    "    y_train_dict = {}\n",
    "    y_val_dict = {}\n",
    "\n",
    "    for batch_size in batch_sizes:\n",
    "        X_train_scaled_dict[batch_size] = []\n",
    "        X_val_scaled_dict[batch_size] = []\n",
    "        y_train_dict[batch_size] = []\n",
    "        y_val_dict[batch_size] = []\n",
    "    \n",
    "    X_train = X_train[:, 1:]\n",
    "    for train_idx, val_idx in cv.split(X_train):\n",
    "        X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        standard_scaler = preprocessing.StandardScaler()\n",
    "        X_train_fold_scaled = standard_scaler.fit_transform(X_train_fold)\n",
    "        X_val_fold_scaled = standard_scaler.fit_transform(X_val_fold)\n",
    "        \n",
    "        for batch_size in batch_sizes:\n",
    "            X_train_scaled_dict[batch_size].append(X_train_fold_scaled)\n",
    "            X_val_scaled_dict[batch_size].append(X_val_fold_scaled)\n",
    "            y_train_dict[batch_size].append(y_train_fold)\n",
    "            y_val_dict[batch_size].append(y_val_fold)\n",
    "    \n",
    "    return X_train_scaled_dict, X_val_scaled_dict, y_train_dict, y_val_dict\n",
    "\n",
    "batch_sizes = [128,256,512,1024]\n",
    "X_train_scaled_dict, X_val_scaled_dict, y_train_dict, y_val_dict = generate_cv_folds_for_batch_sizes(batch_sizes, X_train.to_numpy(), y_train)\n",
    "# sanity check: note that 6751 / 8439 is around 80%\n",
    "# print(X_train_scaled_dict)\n",
    "print(len(X_train_scaled_dict))\n",
    "print(len(X_train_scaled_dict[128]))\n",
    "print(len(X_train_scaled_dict[128][0]))\n",
    "print(len(X_train_scaled_dict[256][2][0]))\n",
    "print(X_train_scaled_dict[128][0])\n",
    "\n",
    "# sanity check: this is the other 20%\n",
    "# print(X_val_scaled_dict)\n",
    "print(len(X_val_scaled_dict))\n",
    "print(len(X_val_scaled_dict[128]))\n",
    "print(len(X_val_scaled_dict[128][0]))\n",
    "print(len(X_val_scaled_dict[256][2][0]))\n",
    "print(X_val_scaled_dict[128][0])\n",
    "\n",
    "# print(y_train_dict)\n",
    "print(len(y_train_dict))\n",
    "print(len(y_train_dict[128]))\n",
    "print(len(y_train_dict[128][0]))\n",
    "print(y_train_dict[128][0])\n",
    "print(y_train_dict[128][0][1])\n",
    "\n",
    "# print(y_val_dict)\n",
    "print(len(y_val_dict))\n",
    "print(len(y_val_dict[128]))\n",
    "print(len(y_val_dict[128][0]))\n",
    "print(y_val_dict[128][0])\n",
    "print(y_val_dict[128][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eb7c89",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "235ca332-9676-42bd-9801-0f5f4157a777",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ae5f281cd84f4d36f81f2ae126cf915",
     "grade": true,
     "grade_id": "correct_dataset",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a03f63",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "8df744af-f485-4871-9e0a-70fd41d1df4d",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4dcf6be1ad49306172e6f27243e613f2",
     "grade": true,
     "grade_id": "correct_dataset2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b78c098",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "558aa470-6d7e-454c-9cda-9ad881d58c53",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "064d68c9708b5e3f1e2463001b6d78b4",
     "grade": false,
     "grade_id": "a2_1_4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "4. Perform hyperparameter tuning for the different batch sizes with 5-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "25aca0e6",
   "metadata": {
    "deletable": false,
    "id": "3107ebe9-d121-4510-9782-2a62d32258d0",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e9665887943f38ae7bed6c1d8351903b",
     "grade": true,
     "grade_id": "hyperparameter_tuning",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[92], line 128\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cross_validation_accuracies, cross_validation_times\n\u001b[0;32m    127\u001b[0m batch_sizes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m128\u001b[39m,\u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m512\u001b[39m,\u001b[38;5;241m1024\u001b[39m]\n\u001b[1;32m--> 128\u001b[0m cross_validation_accuracies, cross_validation_times \u001b[38;5;241m=\u001b[39m \u001b[43mfind_optimal_hyperparameter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val_scaled_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[92], line 83\u001b[0m, in \u001b[0;36mfind_optimal_hyperparameter\u001b[1;34m(X_train_scaled_dict, X_val_scaled_dict, y_train_dict, y_val_dict, batch_sizes)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m     82\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 83\u001b[0m     acc_ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_loop_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m     end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;66;03m# for a fold, the list of accuracies for the batches of that epoch^\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[92], line 31\u001b[0m, in \u001b[0;36mtrain_loop_batch\u001b[1;34m(dataloader, model, loss_fn, optimizer, x_test, y_test)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[0;32m     30\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 31\u001b[0m         \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m#         print(len(x_test[0]))\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def intialise_loaders_batch(X_train_scaled, y_train, X_test_scaled, y_test, batch_size):\n",
    "\n",
    "#     print(\"X_train_scaled in initialise loaders batch\")\n",
    "#     print(len(X_train_scaled[0]))\n",
    "    train_data = BatchCustomDataset(X_train_scaled,y_train)\n",
    "#     print(len(train_data[1]))\n",
    "    test_data = BatchCustomDataset(X_test_scaled,y_test)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "def train_loop_batch(dataloader, model, loss_fn, optimizer, x_test, y_test):\n",
    "    # put within the epochs loop\n",
    "#     size = len(dataloader.dataset)\n",
    "#     num_batches = len(dataloader)\n",
    "#     print(size)\n",
    "#     print(num_batches)\n",
    "#     train_loss, train_correct = 0, 0\n",
    "    acc_ = []\n",
    "#     print\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "#         print(len(X[0]))\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         print(len(x_test[0]))\n",
    "        pred = model(torch.tensor(x_test, dtype=torch.float))\n",
    "#         print(pred)\n",
    "#         print(y_test)\n",
    "#         acc__ = (pred.argmax(1) == torch.tensor(y_test, dtype=torch.float).argmax(1)).type(torch.float).mean()\n",
    "        acc__ = (pred.argmax(1) == torch.tensor(y_test, dtype=torch.float)).type(torch.float).mean()\n",
    "        \n",
    "        acc_.append(acc__.item())\n",
    "        \n",
    "    return acc_\n",
    "#         train_loss += loss.item()\n",
    "#         train_correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    \n",
    "#     train_loss /= num_batches\n",
    "#     train_correct /=size\n",
    "\n",
    "#     return train_loss, train_correct\n",
    "\n",
    "# YOUR CODE HERE\n",
    "def find_optimal_hyperparameter(X_train_scaled_dict, X_val_scaled_dict, y_train_dict, y_val_dict, batch_sizes):\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    \n",
    "    model = MLP(77,128,2)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    cross_validation_times = []\n",
    "#     acc = []\n",
    "    foldaccuracyofabatchsize = []\n",
    "    timeforafoldforthatbatchsize = []\n",
    "    meanaccuracyofabatchsizelist = []\n",
    "    meantimeofabatchsizelist = []\n",
    "    for batch_size in batch_sizes:\n",
    "        print(foldaccuracyofabatchsize)\n",
    "        print(timeforafoldforthatbatchsize)\n",
    "        foldaccuracyofabatchsize = []\n",
    "        timeforafoldforthatbatchsize = []\n",
    "        for idxy in range(0,5):\n",
    "            x_train = X_train_scaled_dict[batch_size][idxy]\n",
    "            y_train = y_train_dict[batch_size][idxy]\n",
    "            x_test = X_val_scaled_dict[batch_size][idxy]\n",
    "            y_test = y_val_dict[batch_size][idxy]\n",
    "            \n",
    "#             acc_ = []\n",
    "#             time = []\n",
    "#             print(len(x_train[0]))\n",
    "            train_dataloader, test_dataloader = intialise_loaders_batch(x_train, y_train, x_test, y_test, batch_size)\n",
    "#             print(train_dataloader.dataset)\n",
    "            for epoch in range(100):\n",
    "                start = time.time()\n",
    "                acc_ = train_loop_batch(train_dataloader, model, loss_fn, optimizer, x_test, y_test)\n",
    "                end = time.time()\n",
    "                # for a fold, the list of accuracies for the batches of that epoch^\n",
    "                if epoch==100:\n",
    "                    foldaccuracyofabatchsize.append(np.mean(np.array(acc_), axis = 0))\n",
    "#                     the accuracy for the last epoch of that fold - the fold accuracy for that batch size, length is 5\n",
    "                    timeforafoldforthatbatchsize.append(end-start)\n",
    "        meanaccuracyofabatchsizelist.append(np.mean(np.array(foldaccuracyofabatchsize), axis = 0))\n",
    "        meantimeofabatchsizelist.append(np.mean(np.array(timeforafoldforthatbatchsize), axis = 0))\n",
    "        # length should be 4^\n",
    "\n",
    "#         acc_ = []\n",
    "#         for no_hidden in hidden_units:\n",
    "        \n",
    "#             model = FFN(no_inputs, no_hidden, no_outputs)\n",
    "    \n",
    "#             loss_fn = torch.nn.CrossEntropyLoss()\n",
    "#             optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "#             for epoch in range(100):\n",
    "#                 pred = model(torch.tensor(x_train, dtype=torch.float))\n",
    "#                 loss = loss_fn(pred, torch.tensor(y_train, dtype=torch.float))\n",
    "    \n",
    "#                 # Backpropagation\n",
    "#                 optimizer.zero_grad()\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "    \n",
    "#             pred = model(torch.tensor(x_test, dtype=torch.float))\n",
    "#             acc__ = (pred.argmax(1) == torch.tensor(y_test, dtype=torch.float).argmax(1)).type(torch.float).mean()\n",
    "    \n",
    "#             acc_.append(acc__.item())\n",
    "#             accuracylistfor5foldsforabatchsize.append(acc_)\n",
    "#         acc.append(accuracylistfor5foldsforabatchsize.mean)\n",
    "#         acc.append(acc_)\n",
    "    \n",
    "#     cv_acc = np.mean(np.array(acc), axis = 0)\n",
    "#     cross_validation_accuracies = cv_acc\n",
    "    cross_validation_accuracies = meanaccuracyofabatchsizelist\n",
    "    cross_validation_times = meantimeofabatchsizelist\n",
    "    print(cross_validation_accuracies)\n",
    "    print(cross_validation_times)\n",
    "    return cross_validation_accuracies, cross_validation_times\n",
    "\n",
    "batch_sizes = [128,256,512,1024]\n",
    "cross_validation_accuracies, cross_validation_times = find_optimal_hyperparameter(X_train_scaled_dict, X_val_scaled_dict, y_train_dict, y_val_dict, batch_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099e7048",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "64384c9c-ddd5-4460-bf37-b9977443a65c",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "975e552e751c4efb2cec0eac214f85cd",
     "grade": true,
     "grade_id": "correct_hyperparameter_tuning",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65f9e17c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "b6756ab6-92e0-4a5e-b4b9-aebe009f5480",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "69421943e22521de848bb03a50f57767",
     "grade": false,
     "grade_id": "a2_1_5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "5. Plot scatterplot of mean cross validation accuracies for the different batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1cbca2",
   "metadata": {
    "deletable": false,
    "id": "8fa3afdf-eed6-47b9-9acc-bc2304c46ec3",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17599eb29fd6e3a1e2812f0ff7cba983",
     "grade": true,
     "grade_id": "plot",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f391ee",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "baab6e4d-4e8b-4358-a68d-682f60db4a06",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "11e8d298b5774c4044f1c3f950c46214",
     "grade": false,
     "grade_id": "a2_1_6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "6. Create a table of time taken to train the network on the last epoch against different batch sizes. Select the optimal batch size and state a reason for your selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a7721d",
   "metadata": {
    "deletable": false,
    "id": "081aa567-cd92-4749-93fd-fc6608a1f6ae",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c18e30a9850c282ad725336848222a62",
     "grade": false,
     "grade_id": "times",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Batch Size':\n",
    "                   'Last Epoch Time':\n",
    "                  })\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ee61aa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "1c83d786-706b-46d2-9220-3b09e4c473b3",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a2fc4a52c2a0af7ea586ea85cec9b3e9",
     "grade": true,
     "grade_id": "correct_times",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f593acfa",
   "metadata": {
    "deletable": false,
    "id": "d46dfd1c-1d3c-46e4-98d6-21c2672ad31b",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "38690f32ec506325fc73c8353b77d041",
     "grade": false,
     "grade_id": "batch_size",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "optimal_batch_size =\n",
    "reason ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa0474c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "096ff7b5-6a77-47d4-941e-37bc495b6558",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f695b961ed43ec6a31b7647e078fd8d6",
     "grade": true,
     "grade_id": "correct_batch_size",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
