{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFVxWZGJxprU"
   },
   "source": [
    "# CS4001/4042 Assignment 1, Part B, Q2\n",
    "In Question B1, we used the Category Embedding model. This creates a feedforward neural network in which the categorical features get learnable embeddings. In this question, we will make use of a library called Pytorch-WideDeep. This library makes it easy to work with multimodal deep-learning problems combining images, text, and tables. We will just be utilizing the deeptabular component of this library through the TabMlp network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EycCozG06Duu"
   },
   "outputs": [],
   "source": [
    "!pip install pytorch-widedeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lq0elU0J53Yo"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "import os\n",
    "\n",
    "import random\n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pytorch_widedeep.preprocessing import TabPreprocessor\n",
    "from pytorch_widedeep.models import TabMlp, WideDeep\n",
    "from pytorch_widedeep import Trainer\n",
    "from pytorch_widedeep.metrics import R2Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aU3xdVpwzuLx"
   },
   "source": [
    ">Divide the dataset (‘hdb_price_prediction.csv’) into train and test sets by using entries from the year 2020 and before as training data, and entries from 2021 and after as the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_oYG6lNIh7Mp"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('hdb_price_prediction.csv')\n",
    "\n",
    "df2021andaftertest = df[df['year']>=2021]\n",
    "df2020andbeforetrain = df[df['year']<=2020]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_q9PoR50JAA"
   },
   "source": [
    ">Refer to the documentation of Pytorch-WideDeep and perform the following tasks:\n",
    "https://pytorch-widedeep.readthedocs.io/en/latest/index.html\n",
    "* Use [**TabPreprocessor**](https://pytorch-widedeep.readthedocs.io/en/latest/examples/01_preprocessors_and_utils.html#2-tabpreprocessor) to create the deeptabular component using the continuous\n",
    "features and the categorical features. Use this component to transform the training dataset.\n",
    "* Create the [**TabMlp**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/model_components.html#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp) model with 2 linear layers in the MLP, with 200 and 100 neurons respectively.\n",
    "* Create a [**Trainer**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/trainer.html#pytorch_widedeep.training.Trainer) for the training of the created TabMlp model with the root mean squared error (RMSE) cost function. Train the model for 100 epochs using this trainer, keeping a batch size of 64. (Note: set the *num_workers* parameter to 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ZBY1iqUXtYWn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gareth Thong\\anaconda3\\envs\\nnb2\\lib\\site-packages\\pytorch_widedeep\\preprocessing\\tab_preprocessor.py:334: UserWarning: Continuous columns will not be normalised\n",
      "  warnings.warn(\"Continuous columns will not be normalised\")\n",
      "epoch 1: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:13<00:00, 102.09it/s, loss=2.22e+5]\n",
      "epoch 2: 100%|███████████████████████████████████████████████████████| 1366/1366 [00:14<00:00, 92.78it/s, loss=9.66e+4]\n",
      "epoch 3: 100%|███████████████████████████████████████████████████████| 1366/1366 [00:15<00:00, 88.70it/s, loss=8.58e+4]\n",
      "epoch 4: 100%|███████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 80.17it/s, loss=7.99e+4]\n",
      "epoch 5: 100%|███████████████████████████████████████████████████████| 1366/1366 [00:16<00:00, 81.34it/s, loss=7.63e+4]\n",
      "epoch 6: 100%|███████████████████████████████████████████████████████| 1366/1366 [00:16<00:00, 84.00it/s, loss=7.33e+4]\n",
      "epoch 7: 100%|███████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 77.51it/s, loss=7.11e+4]\n",
      "epoch 8: 100%|██████████████████████████████████████████████████████████| 1366/1366 [00:16<00:00, 81.32it/s, loss=7e+4]\n",
      "epoch 9: 100%|███████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 80.07it/s, loss=6.89e+4]\n",
      "epoch 10: 100%|███████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 79.97it/s, loss=6.8e+4]\n",
      "epoch 11: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 78.35it/s, loss=6.73e+4]\n",
      "epoch 12: 100%|███████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 78.68it/s, loss=6.7e+4]\n",
      "epoch 13: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 78.24it/s, loss=6.63e+4]\n",
      "epoch 14: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:18<00:00, 74.40it/s, loss=6.61e+4]\n",
      "epoch 15: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 78.76it/s, loss=6.59e+4]\n",
      "epoch 16: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 78.58it/s, loss=6.56e+4]\n",
      "epoch 17: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 77.64it/s, loss=6.52e+4]\n",
      "epoch 18: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 78.14it/s, loss=6.49e+4]\n",
      "epoch 19: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 78.20it/s, loss=6.47e+4]\n",
      "epoch 20: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:16<00:00, 83.75it/s, loss=6.46e+4]\n",
      "epoch 21: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:16<00:00, 84.19it/s, loss=6.43e+4]\n",
      "epoch 22: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:16<00:00, 85.10it/s, loss=6.43e+4]\n",
      "epoch 23: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:16<00:00, 81.07it/s, loss=6.37e+4]\n",
      "epoch 24: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:16<00:00, 81.41it/s, loss=6.38e+4]\n",
      "epoch 25: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:16<00:00, 80.80it/s, loss=6.38e+4]\n",
      "epoch 26: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:16<00:00, 82.45it/s, loss=6.34e+4]\n",
      "epoch 27: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:16<00:00, 82.57it/s, loss=6.32e+4]\n",
      "epoch 28: 100%|███████████████████████████████████████████████████████| 1366/1366 [00:16<00:00, 81.29it/s, loss=6.3e+4]\n",
      "epoch 29: 100%|███████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 80.17it/s, loss=6.3e+4]\n",
      "epoch 30: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 79.93it/s, loss=6.26e+4]\n",
      "epoch 31: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 77.60it/s, loss=6.28e+4]\n",
      "epoch 32: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:16<00:00, 81.80it/s, loss=6.25e+4]\n",
      "epoch 33: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 78.51it/s, loss=6.23e+4]\n",
      "epoch 34: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:16<00:00, 82.37it/s, loss=6.21e+4]\n",
      "epoch 35: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:16<00:00, 85.34it/s, loss=6.21e+4]\n",
      "epoch 36: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:16<00:00, 84.35it/s, loss=6.19e+4]\n",
      "epoch 37: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:16<00:00, 85.19it/s, loss=6.18e+4]\n",
      "epoch 38: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:15<00:00, 88.84it/s, loss=6.14e+4]\n",
      "epoch 39: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:16<00:00, 80.60it/s, loss=6.13e+4]\n",
      "epoch 40: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:18<00:00, 75.81it/s, loss=6.15e+4]\n",
      "epoch 41: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:19<00:00, 71.86it/s, loss=6.14e+4]\n",
      "epoch 42: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 79.72it/s, loss=6.09e+4]\n",
      "epoch 43: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 80.00it/s, loss=6.08e+4]\n",
      "epoch 44: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:18<00:00, 74.76it/s, loss=6.08e+4]\n",
      "epoch 45: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 76.48it/s, loss=6.07e+4]\n",
      "epoch 46: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:19<00:00, 70.96it/s, loss=6.06e+4]\n",
      "epoch 47: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 79.57it/s, loss=6.03e+4]\n",
      "epoch 48: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:18<00:00, 73.02it/s, loss=6.02e+4]\n",
      "epoch 49: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:18<00:00, 73.33it/s, loss=6.01e+4]\n",
      "epoch 50: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 77.21it/s, loss=5.99e+4]\n",
      "epoch 51: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 79.04it/s, loss=5.97e+4]\n",
      "epoch 52: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:20<00:00, 68.21it/s, loss=5.96e+4]\n",
      "epoch 53: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 77.94it/s, loss=5.96e+4]\n",
      "epoch 54: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 79.48it/s, loss=5.93e+4]\n",
      "epoch 55: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 78.32it/s, loss=5.94e+4]\n",
      "epoch 56: 100%|███████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 78.30it/s, loss=5.9e+4]\n",
      "epoch 57: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 78.60it/s, loss=5.89e+4]\n",
      "epoch 58: 100%|███████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 78.10it/s, loss=5.9e+4]\n",
      "epoch 59: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 75.97it/s, loss=5.88e+4]\n",
      "epoch 60: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 76.50it/s, loss=5.88e+4]\n",
      "epoch 61: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 77.87it/s, loss=5.89e+4]\n",
      "epoch 62: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 78.44it/s, loss=5.86e+4]\n",
      "epoch 63: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 78.74it/s, loss=5.84e+4]\n",
      "epoch 64: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 78.61it/s, loss=5.81e+4]\n",
      "epoch 65: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 77.58it/s, loss=5.77e+4]\n",
      "epoch 66: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:18<00:00, 75.62it/s, loss=5.75e+4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 67: 100%|███████████████████████████████████████████████████████| 1366/1366 [00:15<00:00, 89.00it/s, loss=5.7e+4]\n",
      "epoch 68: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:15<00:00, 89.29it/s, loss=5.65e+4]\n",
      "epoch 69: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:15<00:00, 89.73it/s, loss=5.59e+4]\n",
      "epoch 70: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:15<00:00, 88.85it/s, loss=5.53e+4]\n",
      "epoch 71: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:15<00:00, 89.01it/s, loss=5.47e+4]\n",
      "epoch 72: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:15<00:00, 89.08it/s, loss=5.38e+4]\n",
      "epoch 73: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:15<00:00, 87.84it/s, loss=5.33e+4]\n",
      "epoch 74: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:15<00:00, 88.39it/s, loss=5.29e+4]\n",
      "epoch 75: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:15<00:00, 87.88it/s, loss=5.26e+4]\n",
      "epoch 76: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:16<00:00, 85.01it/s, loss=5.24e+4]\n",
      "epoch 77: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:15<00:00, 85.78it/s, loss=5.22e+4]\n",
      "epoch 78: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:16<00:00, 83.36it/s, loss=5.17e+4]\n",
      "epoch 79: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:15<00:00, 86.23it/s, loss=5.18e+4]\n",
      "epoch 80: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:15<00:00, 86.08it/s, loss=5.15e+4]\n",
      "epoch 81: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:16<00:00, 82.91it/s, loss=5.16e+4]\n",
      "epoch 82: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:16<00:00, 85.12it/s, loss=5.15e+4]\n",
      "epoch 83: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:16<00:00, 84.08it/s, loss=5.13e+4]\n",
      "epoch 84: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:16<00:00, 83.78it/s, loss=5.11e+4]\n",
      "epoch 85: 100%|███████████████████████████████████████████████████████| 1366/1366 [00:16<00:00, 82.46it/s, loss=5.1e+4]\n",
      "epoch 86: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:16<00:00, 81.91it/s, loss=5.09e+4]\n",
      "epoch 87: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:16<00:00, 83.01it/s, loss=5.09e+4]\n",
      "epoch 88: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 78.61it/s, loss=5.07e+4]\n",
      "epoch 89: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 79.94it/s, loss=5.07e+4]\n",
      "epoch 90: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 79.83it/s, loss=5.06e+4]\n",
      "epoch 91: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:16<00:00, 81.09it/s, loss=5.05e+4]\n",
      "epoch 92: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 79.69it/s, loss=5.05e+4]\n",
      "epoch 93: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 79.84it/s, loss=5.04e+4]\n",
      "epoch 94: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:16<00:00, 80.56it/s, loss=5.03e+4]\n",
      "epoch 95: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:18<00:00, 75.56it/s, loss=5.03e+4]\n",
      "epoch 96: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:20<00:00, 68.10it/s, loss=5.01e+4]\n",
      "epoch 97: 100%|█████████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 79.35it/s, loss=5e+4]\n",
      "epoch 98: 100%|██████████████████████████████████████████████████████| 1366/1366 [00:16<00:00, 81.24it/s, loss=4.99e+4]\n",
      "epoch 99: 100%|█████████████████████████████████████████████████████████| 1366/1366 [00:17<00:00, 80.30it/s, loss=5e+4]\n",
      "epoch 100: 100%|████████████████████████████████████████████████████████| 1366/1366 [00:16<00:00, 80.94it/s, loss=5e+4]\n",
      "predict: 100%|████████████████████████████████████████████████████████████████████| 1128/1128 [00:05<00:00, 209.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# For questions B1 and B2, the following features should be used:    \n",
    "# - Numeric / Continuous features: dist_to_nearest_stn, dist_to_dhoby, degree_centrality, eigenvector_centrality, remaining_lease_years, floor_area_sqm \n",
    "# - Categorical features: month, town, flat_model_type, storey_range\n",
    "\n",
    "target = df2020andbeforetrain[\"resale_price\"].values\n",
    "cat_embed_cols = [\"month\", \"town\", \"flat_model_type\", \"storey_range\",]\n",
    "continuous_cols = [\"dist_to_nearest_stn\", \"dist_to_dhoby\", \"degree_centrality\", \"eigenvector_centrality\", \"remaining_lease_years\", \"floor_area_sqm\" ]\n",
    "tab_preprocessor = TabPreprocessor(cat_embed_cols=cat_embed_cols, continuous_cols=continuous_cols)\n",
    "X_tab = tab_preprocessor.fit_transform(df2020andbeforetrain)\n",
    "\n",
    "tab_mlp = TabMlp(\n",
    "    mlp_hidden_dims=[200, 100],\n",
    "    column_idx=tab_preprocessor.column_idx,\n",
    "    cat_embed_input=tab_preprocessor.cat_embed_input,\n",
    "    continuous_cols=continuous_cols,\n",
    ")\n",
    "\n",
    "model = WideDeep(deeptabular=tab_mlp)\n",
    "\n",
    "trainer = Trainer(model, objective=\"root_mean_squared_error\", num_workers=0)\n",
    "trainer.fit(\n",
    "    X_tab=X_tab,\n",
    "    target=target,\n",
    "    n_epochs=100,\n",
    "    batch_size=64,\n",
    ")\n",
    "\n",
    "X_tab_te = tab_preprocessor.transform(df2021andaftertest)\n",
    "preds = trainer.predict(X_tab=X_tab_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V46s-MdM0y5c"
   },
   "source": [
    ">Report the test RMSE and the test R2 value that you obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "KAhAgvMC07g6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test RMSE value is 97224.1841396755\n",
      "The test R2 value is 0.6697467159129393\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "rmse = mean_squared_error(df2021andaftertest[\"resale_price\"], preds, squared=False)\n",
    "print(\"The test RMSE value is \" + str(rmse))\n",
    "\n",
    "r2value = r2_score(df2021andaftertest[\"resale_price\"], preds)\n",
    "print(\"The test R2 value is \" + str(r2value))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
