{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4ed53dd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "c5cb62ac-8e88-43e6-bce9-da20fabf38ff",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "987c7c95a0c7dc71b3d85e154cc3a9be",
     "grade": false,
     "grade_id": "cell-6ebb8bd2f22353d3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Question A4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa81c91",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5c8f824c",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "17d770ae590711dc06f03d150970a3f1",
     "grade": false,
     "grade_id": "cell-e34b0415c38ebac4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this section, we will understand the utility of such a neural network in real world scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98b4da1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "fb9411ad-2324-400e-852e-ff5c0ca716f0",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "287259c58079728b66dae175c6082400",
     "grade": false,
     "grade_id": "cell-4f74b97314b65ea1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "#### Please use the real record data named ‘record.wav’  as a test sample. Preprocess the data using the provided preprocessing script (data_preprocess.ipynb) and prepare the dataset.\n",
    "Do a model prediction on the sample test dataset and obtain the predicted label using a threshold of 0.5. The model used is the optimized pretrained model using the selected optimal batch size and optimal number of neurons.\n",
    "Find the most important features on the model prediction for the test sample using SHAP. Plot the local feature importance with a force plot and explain your observations.  (Refer to the documentation and these three useful references:\n",
    "https://christophm.github.io/interpretable-ml-book/shap.html#examples-5,\n",
    "https://towardsdatascience.com/deep-learning-model-interpretation-using-shap-a21786e91d16,  \n",
    "https://medium.com/mlearning-ai/shap-force-plots-for-classification-d30be430e195)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996766fc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "981c85ca-9a14-4d0a-b44d-814f02c0f8e1",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "30c3b93836aad148380e15933e7dd786",
     "grade": false,
     "grade_id": "cell-b8a265bf37e3b271",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "1. Firstly, we import relevant libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48c7450b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "58c50f4f",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8f6af6091e2832c850b00e735d1cff11",
     "grade": false,
     "grade_id": "libraries",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from scipy.io import wavfile as wav\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "from common_utils import set_seed\n",
    "\n",
    "# setting seed\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e0d8f7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "d3444c83",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d796a3a33dd56bd5afb55de45b642449",
     "grade": false,
     "grade_id": "cell-293c9e85ad81d29a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To reduce repeated code, place your\n",
    "network (MLP defined in QA1)\n",
    "torch datasets (CustomDataset defined in QA1)\n",
    "loss function (loss_fn defined in QA1)\n",
    "in a separate file called common_utils.py\n",
    "\n",
    "Import them into this file. You will not be repenalised for any error in QA1 here as the code in QA1 will not be remarked.\n",
    "\n",
    "The following code cell will not be marked.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eef99838",
   "metadata": {
    "deletable": false,
    "id": "72e8e840",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0c623c0417cb6065d1bbb049f211cf1c",
     "grade": false,
     "grade_id": "cell-29dace0045a28b89",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import json\n",
    "\n",
    "def extract_features(filepath):\n",
    "    \n",
    "    '''\n",
    "    Source: https://github.com/danz1ka19/Music-Emotion-Recognition/blob/master/Feature-Extraction.py\n",
    "    Modified to process a single file\n",
    "\n",
    "        function: extract_features\n",
    "        input: path to mp3 files\n",
    "        output: csv file containing features extracted\n",
    "\n",
    "        This function reads the content in a directory and for each audio file detected\n",
    "        reads the file and extracts relevant features using librosa library for audio\n",
    "        signal processing\n",
    "    '''\n",
    "\n",
    "    feature_set = {}  # Features\n",
    "\n",
    "    # Reading audio file\n",
    "    y, sr = librosa.load(filepath)\n",
    "    S = np.abs(librosa.stft(y, n_fft=512)) \n",
    "    # https://librosa.org/doc/main/generated/librosa.stft.html (set 512 for speech processing)\n",
    "\n",
    "    # Extracting Features\n",
    "    tempo, beats = librosa.beat.beat_track(y=y, sr=sr)\n",
    "    chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr, n_fft=512)\n",
    "    \n",
    "    chroma_cq = librosa.feature.chroma_cqt(y=y, sr=sr)\n",
    "    \n",
    "    chroma_cens = librosa.feature.chroma_cens(y=y, sr=sr)\n",
    "    melspectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=512)\n",
    "    rmse = librosa.feature.rms(y=y)[0]\n",
    "    cent = librosa.feature.spectral_centroid(y=y, sr=sr, n_fft=512)\n",
    "    spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr, n_fft=512)\n",
    "    contrast = librosa.feature.spectral_contrast(S=S, sr=sr, n_fft=512)\n",
    "    rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr, n_fft=512)\n",
    "    poly_features = librosa.feature.poly_features(S=S, sr=sr, n_fft=512)\n",
    "    \n",
    "    tonnetz = librosa.feature.tonnetz(y=y, sr=sr)\n",
    "    \n",
    "    zcr = librosa.feature.zero_crossing_rate(y)\n",
    "    harmonic = librosa.effects.harmonic(y)\n",
    "    percussive = librosa.effects.percussive(y)\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_fft=512)\n",
    "    mfcc_delta = librosa.feature.delta(mfcc)\n",
    "\n",
    "    onset_frames = librosa.onset.onset_detect(y=y, sr=sr)\n",
    "    frames_to_time = librosa.frames_to_time(onset_frames[:20], sr=sr)\n",
    "\n",
    "    # Concatenating Features into one csv and json format\n",
    "    feature_set['filename'] = filepath  # song name\n",
    "    feature_set['tempo'] = tempo  # tempo \n",
    "    feature_set['total_beats'] = sum(beats)  # beats\n",
    "    feature_set['average_beats'] = np.average(beats)\n",
    "    feature_set['chroma_stft_mean'] = np.mean(chroma_stft)  # chroma stft\n",
    "    feature_set['chroma_stft_var'] = np.var(chroma_stft)\n",
    "    \n",
    "    feature_set['chroma_cq_mean'] = np.mean(chroma_cq)  # chroma cq\n",
    "    feature_set['chroma_cq_var'] = np.var(chroma_cq)\n",
    "    \n",
    "    feature_set['chroma_cens_mean'] = np.mean(chroma_cens)  # chroma cens\n",
    "    feature_set['chroma_cens_var'] = np.var(chroma_cens)\n",
    "    feature_set['melspectrogram_mean'] = np.mean(melspectrogram)  # melspectrogram\n",
    "    feature_set['melspectrogram_var'] = np.var(melspectrogram)\n",
    "    feature_set['mfcc_mean'] = np.mean(mfcc)  # mfcc\n",
    "    feature_set['mfcc_var'] = np.var(mfcc)\n",
    "    feature_set['mfcc_delta_mean'] = np.mean(mfcc_delta)  # mfcc delta\n",
    "    feature_set['mfcc_delta_var'] = np.var(mfcc_delta)\n",
    "    feature_set['rmse_mean'] = np.mean(rmse)  # rmse\n",
    "    feature_set['rmse_var'] = np.var(rmse)\n",
    "    feature_set['cent_mean'] = np.mean(cent)  # cent\n",
    "    feature_set['cent_var'] = np.var(cent)\n",
    "    feature_set['spec_bw_mean'] = np.mean(spec_bw)  # spectral bandwidth\n",
    "    feature_set['spec_bw_var'] = np.var(spec_bw)\n",
    "    feature_set['contrast_mean'] = np.mean(contrast)  # contrast\n",
    "    feature_set['contrast_var'] = np.var(contrast)\n",
    "    feature_set['rolloff_mean'] = np.mean(rolloff)  # rolloff\n",
    "    feature_set['rolloff_var'] = np.mean(rolloff)\n",
    "    feature_set['poly_mean'] = np.mean(poly_features)  # poly features\n",
    "    feature_set['poly_var'] = np.mean(poly_features)\n",
    "    \n",
    "    feature_set['tonnetz_mean'] = np.mean(tonnetz)  # tonnetz\n",
    "    feature_set['tonnetz_var'] = np.var(tonnetz)\n",
    "    \n",
    "    feature_set['zcr_mean'] = np.mean(zcr)  # zero crossing rate\n",
    "    feature_set['zcr_var'] = np.var(zcr)\n",
    "    feature_set['harm_mean'] = np.mean(harmonic)  # harmonic\n",
    "    feature_set['harm_var'] = np.var(harmonic)\n",
    "    feature_set['perc_mean'] = np.mean(percussive)  # percussive\n",
    "    feature_set['perc_var'] = np.var(percussive)\n",
    "    feature_set['frame_mean'] = np.mean(frames_to_time)  # frames\n",
    "    feature_set['frame_var'] = np.var(frames_to_time)\n",
    "    \n",
    "    for ix, coeff in enumerate(mfcc):\n",
    "        feature_set['mfcc' + str(ix) + '_mean'] = coeff.mean()\n",
    "        feature_set['mfcc' + str(ix) + '_var'] = coeff.var()\n",
    "    \n",
    "    return feature_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b870a7f4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "b12f3ced-a6a1-4628-a409-1ca7bdfd1cfa",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8dbab024c3394801484199efdbbdb269",
     "grade": true,
     "grade_id": "corrected",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a84ee778",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "18fd5d5e",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7da5539e4fe97549a11c7d61be647167",
     "grade": false,
     "grade_id": "cell-1c5bf554b8f89a3d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "2. Install and import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b40347e",
   "metadata": {
    "deletable": false,
    "id": "e49be1fc",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f58a0104d88201d0af7de9fc3a6ca035",
     "grade": false,
     "grade_id": "import_shap",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3fd960",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ef497933-2108-4aa5-8ec8-5729214cb1cd",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4cf4df5a01325e8ea1f585dcfc81b01b",
     "grade": true,
     "grade_id": "import_shap_correct",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39337ffe",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "c5fde60a",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b8877105a451813ab23b45e9a180bc36",
     "grade": false,
     "grade_id": "cell-82dd5a271bf5af4b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "3. Read the csv data preprocessed from 'record.wav', using variable name 'df', and fill the size of 'df' in 'size_row' and 'size_column'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f64e869d",
   "metadata": {
    "deletable": false,
    "id": "81a54d47",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c35348846173e5c042d78be10546ae86",
     "grade": false,
     "grade_id": "cell-01d5f7ef70e69e09",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "new_features_dict = extract_features('record.wav')\n",
    "df = pd.DataFrame([new_features_dict])\n",
    "df.to_csv('./new_record.csv', index=False)\n",
    "\n",
    "df = pd.read_csv('new_record.csv')\n",
    "size_row, size_column = df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2716e64",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "571b0b06-1750-4228-88af-67d8c52035dc",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d137f7e21ec2ea9ad7a57f4411b513a",
     "grade": true,
     "grade_id": "cell-01d5f7ef70e69e0988",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d953f4d9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "558aa470-6d7e-454c-9cda-9ad881d58c53",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d3d13eea6f0ed0d345e10f33dd3a26da",
     "grade": false,
     "grade_id": "cell-7096e580d10284df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    " 4.  Preprocess to obtain the test data, save the test data as numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22017114",
   "metadata": {
    "deletable": false,
    "id": "8c77bd18-c546-473e-8c2f-643b4281d9ba",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b19be33055efd5fc5d562a9c671b6eb2",
     "grade": false,
     "grade_id": "cell-b1e392e8ecab207a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X has feature names, but StandardScaler was fitted without feature names\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train_accuracy: 51.55%, Train_loss: 0.690996, Test_accuracy: 52.65%, Test_loss: 0.689275\n",
      "Epoch 2: Train_accuracy: 55.48%, Train_loss: 0.684479, Test_accuracy: 55.61%, Test_loss: 0.683961\n",
      "Epoch 3: Train_accuracy: 58.12%, Train_loss: 0.677480, Test_accuracy: 57.74%, Test_loss: 0.675777\n",
      "Epoch 4: Train_accuracy: 58.75%, Train_loss: 0.669288, Test_accuracy: 58.90%, Test_loss: 0.670894\n",
      "Epoch 5: Train_accuracy: 60.32%, Train_loss: 0.662362, Test_accuracy: 59.54%, Test_loss: 0.668089\n",
      "Epoch 6: Train_accuracy: 61.90%, Train_loss: 0.652254, Test_accuracy: 60.31%, Test_loss: 0.665861\n",
      "Epoch 7: Train_accuracy: 62.97%, Train_loss: 0.650646, Test_accuracy: 60.12%, Test_loss: 0.666380\n",
      "Epoch 8: Train_accuracy: 64.12%, Train_loss: 0.637956, Test_accuracy: 62.02%, Test_loss: 0.653357\n",
      "Epoch 9: Train_accuracy: 64.75%, Train_loss: 0.633035, Test_accuracy: 60.78%, Test_loss: 0.662860\n",
      "Epoch 10: Train_accuracy: 64.89%, Train_loss: 0.630436, Test_accuracy: 61.53%, Test_loss: 0.658837\n",
      "Epoch 11: Train_accuracy: 66.22%, Train_loss: 0.623406, Test_accuracy: 62.55%, Test_loss: 0.645038\n",
      "Epoch 12: Train_accuracy: 68.10%, Train_loss: 0.613629, Test_accuracy: 62.55%, Test_loss: 0.648030\n",
      "Epoch 13: Train_accuracy: 68.37%, Train_loss: 0.610793, Test_accuracy: 63.65%, Test_loss: 0.639069\n",
      "Epoch 14: Train_accuracy: 68.62%, Train_loss: 0.603010, Test_accuracy: 63.05%, Test_loss: 0.645312\n",
      "Epoch 15: Train_accuracy: 70.15%, Train_loss: 0.593994, Test_accuracy: 64.29%, Test_loss: 0.636997\n",
      "Epoch 16: Train_accuracy: 70.97%, Train_loss: 0.586697, Test_accuracy: 65.42%, Test_loss: 0.629303\n",
      "Epoch 17: Train_accuracy: 70.90%, Train_loss: 0.584088, Test_accuracy: 64.40%, Test_loss: 0.637590\n",
      "Epoch 18: Train_accuracy: 71.73%, Train_loss: 0.578459, Test_accuracy: 65.28%, Test_loss: 0.628868\n",
      "Epoch 19: Train_accuracy: 73.31%, Train_loss: 0.571007, Test_accuracy: 64.87%, Test_loss: 0.631259\n",
      "Epoch 20: Train_accuracy: 72.75%, Train_loss: 0.566976, Test_accuracy: 66.33%, Test_loss: 0.623323\n",
      "Epoch 21: Train_accuracy: 73.85%, Train_loss: 0.562880, Test_accuracy: 67.14%, Test_loss: 0.617392\n",
      "Epoch 22: Train_accuracy: 73.22%, Train_loss: 0.561821, Test_accuracy: 66.39%, Test_loss: 0.623898\n",
      "Epoch 23: Train_accuracy: 74.07%, Train_loss: 0.557939, Test_accuracy: 67.44%, Test_loss: 0.616470\n",
      "Epoch 24: Train_accuracy: 75.22%, Train_loss: 0.551427, Test_accuracy: 67.86%, Test_loss: 0.607957\n",
      "Epoch 25: Train_accuracy: 75.72%, Train_loss: 0.544662, Test_accuracy: 67.91%, Test_loss: 0.610294\n",
      "Epoch 26: Train_accuracy: 76.18%, Train_loss: 0.540447, Test_accuracy: 67.72%, Test_loss: 0.617145\n",
      "Epoch 27: Train_accuracy: 76.63%, Train_loss: 0.539386, Test_accuracy: 68.66%, Test_loss: 0.603665\n",
      "Epoch 28: Train_accuracy: 76.96%, Train_loss: 0.534501, Test_accuracy: 67.61%, Test_loss: 0.605558\n",
      "Epoch 29: Train_accuracy: 77.56%, Train_loss: 0.531386, Test_accuracy: 68.27%, Test_loss: 0.612864\n",
      "Epoch 30: Train_accuracy: 77.64%, Train_loss: 0.528098, Test_accuracy: 69.49%, Test_loss: 0.598783\n",
      "Epoch 31: Train_accuracy: 77.85%, Train_loss: 0.528220, Test_accuracy: 68.99%, Test_loss: 0.600836\n",
      "Epoch 32: Train_accuracy: 78.26%, Train_loss: 0.519215, Test_accuracy: 69.35%, Test_loss: 0.600193\n",
      "Epoch 33: Train_accuracy: 79.25%, Train_loss: 0.514432, Test_accuracy: 68.63%, Test_loss: 0.605057\n",
      "Epoch 34: Train_accuracy: 79.42%, Train_loss: 0.511865, Test_accuracy: 68.02%, Test_loss: 0.605975\n",
      "Epoch 35: Train_accuracy: 79.29%, Train_loss: 0.512445, Test_accuracy: 69.24%, Test_loss: 0.595784\n",
      "Epoch 36: Train_accuracy: 79.56%, Train_loss: 0.511382, Test_accuracy: 68.82%, Test_loss: 0.602499\n",
      "Epoch 37: Train_accuracy: 79.78%, Train_loss: 0.506760, Test_accuracy: 70.18%, Test_loss: 0.593435\n",
      "Epoch 38: Train_accuracy: 79.97%, Train_loss: 0.507691, Test_accuracy: 70.98%, Test_loss: 0.585786\n",
      "Epoch 39: Train_accuracy: 79.83%, Train_loss: 0.506019, Test_accuracy: 69.54%, Test_loss: 0.592306\n",
      "Epoch 40: Train_accuracy: 80.35%, Train_loss: 0.503455, Test_accuracy: 69.76%, Test_loss: 0.595617\n",
      "Epoch 41: Train_accuracy: 80.93%, Train_loss: 0.501129, Test_accuracy: 69.90%, Test_loss: 0.591983\n",
      "Epoch 42: Train_accuracy: 81.35%, Train_loss: 0.495552, Test_accuracy: 69.79%, Test_loss: 0.595251\n",
      "Epoch 43: Train_accuracy: 81.31%, Train_loss: 0.493427, Test_accuracy: 70.95%, Test_loss: 0.587795\n",
      "Epoch 44: Train_accuracy: 81.05%, Train_loss: 0.494782, Test_accuracy: 70.81%, Test_loss: 0.584056\n",
      "Epoch 45: Train_accuracy: 81.96%, Train_loss: 0.493805, Test_accuracy: 70.59%, Test_loss: 0.590453\n",
      "Epoch 46: Train_accuracy: 82.05%, Train_loss: 0.487353, Test_accuracy: 70.45%, Test_loss: 0.589011\n",
      "Epoch 47: Train_accuracy: 81.85%, Train_loss: 0.492489, Test_accuracy: 71.03%, Test_loss: 0.583727\n",
      "Epoch 48: Train_accuracy: 82.59%, Train_loss: 0.483236, Test_accuracy: 70.59%, Test_loss: 0.585509\n",
      "Epoch 49: Train_accuracy: 82.89%, Train_loss: 0.478063, Test_accuracy: 71.06%, Test_loss: 0.586622\n",
      "Epoch 50: Train_accuracy: 82.52%, Train_loss: 0.483530, Test_accuracy: 71.42%, Test_loss: 0.580177\n",
      "Epoch 51: Train_accuracy: 83.21%, Train_loss: 0.475848, Test_accuracy: 70.90%, Test_loss: 0.581997\n",
      "Epoch 52: Train_accuracy: 83.23%, Train_loss: 0.475700, Test_accuracy: 71.48%, Test_loss: 0.580114\n",
      "Epoch 53: Train_accuracy: 83.41%, Train_loss: 0.473306, Test_accuracy: 71.34%, Test_loss: 0.579969\n",
      "Epoch 54: Train_accuracy: 84.01%, Train_loss: 0.471316, Test_accuracy: 71.56%, Test_loss: 0.579205\n",
      "Epoch 55: Train_accuracy: 84.81%, Train_loss: 0.462726, Test_accuracy: 71.50%, Test_loss: 0.579427\n",
      "Epoch 56: Train_accuracy: 84.54%, Train_loss: 0.463685, Test_accuracy: 72.28%, Test_loss: 0.570655\n",
      "Epoch 57: Train_accuracy: 84.29%, Train_loss: 0.464780, Test_accuracy: 71.53%, Test_loss: 0.581531\n",
      "Epoch 58: Train_accuracy: 84.09%, Train_loss: 0.469983, Test_accuracy: 71.39%, Test_loss: 0.582971\n",
      "Epoch 59: Train_accuracy: 83.88%, Train_loss: 0.471432, Test_accuracy: 72.91%, Test_loss: 0.564902\n",
      "Epoch 60: Train_accuracy: 85.20%, Train_loss: 0.458699, Test_accuracy: 71.12%, Test_loss: 0.580585\n",
      "Epoch 61: Train_accuracy: 85.10%, Train_loss: 0.464206, Test_accuracy: 73.08%, Test_loss: 0.565066\n",
      "Epoch 62: Train_accuracy: 84.67%, Train_loss: 0.463014, Test_accuracy: 73.47%, Test_loss: 0.566560\n",
      "Epoch 63: Train_accuracy: 85.18%, Train_loss: 0.461556, Test_accuracy: 73.91%, Test_loss: 0.563341\n",
      "Epoch 64: Train_accuracy: 85.56%, Train_loss: 0.454257, Test_accuracy: 72.78%, Test_loss: 0.574840\n",
      "Epoch 65: Train_accuracy: 85.25%, Train_loss: 0.460622, Test_accuracy: 73.08%, Test_loss: 0.568456\n",
      "Epoch 66: Train_accuracy: 85.24%, Train_loss: 0.452570, Test_accuracy: 71.42%, Test_loss: 0.578853\n",
      "Epoch 67: Train_accuracy: 85.24%, Train_loss: 0.458334, Test_accuracy: 72.55%, Test_loss: 0.573307\n",
      "Epoch 68: Train_accuracy: 85.93%, Train_loss: 0.452996, Test_accuracy: 73.69%, Test_loss: 0.564451\n",
      "Epoch 69: Train_accuracy: 85.56%, Train_loss: 0.454954, Test_accuracy: 72.72%, Test_loss: 0.568192\n",
      "Epoch 70: Train_accuracy: 85.97%, Train_loss: 0.449250, Test_accuracy: 74.65%, Test_loss: 0.558169\n",
      "Epoch 71: Train_accuracy: 86.25%, Train_loss: 0.447296, Test_accuracy: 72.39%, Test_loss: 0.573188\n",
      "Epoch 72: Train_accuracy: 85.84%, Train_loss: 0.450067, Test_accuracy: 72.47%, Test_loss: 0.572010\n",
      "Epoch 73: Train_accuracy: 85.96%, Train_loss: 0.449685, Test_accuracy: 72.11%, Test_loss: 0.578997\n",
      "Epoch 74: Train_accuracy: 86.20%, Train_loss: 0.441933, Test_accuracy: 73.49%, Test_loss: 0.565464\n",
      "Epoch 75: Train_accuracy: 87.04%, Train_loss: 0.436975, Test_accuracy: 73.13%, Test_loss: 0.569554\n",
      "Epoch 76: Train_accuracy: 87.04%, Train_loss: 0.440746, Test_accuracy: 72.97%, Test_loss: 0.569120\n",
      "Epoch 77: Train_accuracy: 86.34%, Train_loss: 0.442335, Test_accuracy: 72.50%, Test_loss: 0.572686\n",
      "Epoch 78: Train_accuracy: 87.34%, Train_loss: 0.438962, Test_accuracy: 73.52%, Test_loss: 0.562410\n",
      "Epoch 79: Train_accuracy: 87.34%, Train_loss: 0.437746, Test_accuracy: 73.71%, Test_loss: 0.564905\n",
      "Epoch 80: Train_accuracy: 87.29%, Train_loss: 0.435146, Test_accuracy: 72.75%, Test_loss: 0.569434\n",
      "Epoch 81: Train_accuracy: 87.75%, Train_loss: 0.436644, Test_accuracy: 73.55%, Test_loss: 0.565139\n",
      "Epoch 82: Train_accuracy: 87.75%, Train_loss: 0.432363, Test_accuracy: 73.94%, Test_loss: 0.562602\n",
      "Epoch 83: Train_accuracy: 87.00%, Train_loss: 0.441278, Test_accuracy: 73.47%, Test_loss: 0.561744\n",
      "Epoch 84: Train_accuracy: 87.97%, Train_loss: 0.429073, Test_accuracy: 73.88%, Test_loss: 0.561623\n",
      "Epoch 85: Train_accuracy: 87.62%, Train_loss: 0.435307, Test_accuracy: 74.10%, Test_loss: 0.558407\n",
      "Epoch 86: Train_accuracy: 88.10%, Train_loss: 0.429292, Test_accuracy: 72.91%, Test_loss: 0.565274\n",
      "Epoch 87: Train_accuracy: 88.34%, Train_loss: 0.425099, Test_accuracy: 72.72%, Test_loss: 0.568983\n",
      "Epoch 88: Train_accuracy: 88.06%, Train_loss: 0.430467, Test_accuracy: 73.88%, Test_loss: 0.561628\n",
      "Epoch 89: Train_accuracy: 88.41%, Train_loss: 0.427800, Test_accuracy: 73.30%, Test_loss: 0.563051\n",
      "Epoch 90: Train_accuracy: 88.30%, Train_loss: 0.426469, Test_accuracy: 74.30%, Test_loss: 0.561951\n",
      "Epoch 91: Train_accuracy: 88.26%, Train_loss: 0.429000, Test_accuracy: 73.91%, Test_loss: 0.555625\n",
      "Epoch 92: Train_accuracy: 88.85%, Train_loss: 0.424525, Test_accuracy: 75.48%, Test_loss: 0.550905\n",
      "Epoch 93: Train_accuracy: 88.88%, Train_loss: 0.422214, Test_accuracy: 74.32%, Test_loss: 0.556588\n",
      "Epoch 94: Train_accuracy: 88.35%, Train_loss: 0.425296, Test_accuracy: 73.66%, Test_loss: 0.560947\n",
      "Epoch 95: Train_accuracy: 88.72%, Train_loss: 0.426453, Test_accuracy: 74.32%, Test_loss: 0.560042\n",
      "Epoch 96: Train_accuracy: 88.70%, Train_loss: 0.422607, Test_accuracy: 74.30%, Test_loss: 0.554636\n",
      "Epoch 97: Train_accuracy: 89.22%, Train_loss: 0.420658, Test_accuracy: 75.01%, Test_loss: 0.550582\n",
      "Epoch 98: Train_accuracy: 89.36%, Train_loss: 0.416153, Test_accuracy: 74.82%, Test_loss: 0.553608\n",
      "Epoch 99: Train_accuracy: 89.73%, Train_loss: 0.415210, Test_accuracy: 74.65%, Test_loss: 0.553938\n",
      "Epoch 100: Train_accuracy: 89.30%, Train_loss: 0.420741, Test_accuracy: 73.80%, Test_loss: 0.559928\n"
     ]
    }
   ],
   "source": [
    "from common_utils import preprocess_dataset, split_dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "def preprocess(X_train, df):\n",
    "    \"\"\"preprocess your dataset to obtain your test dataset, remember to remove the 'filename' as Q1\n",
    "    \"\"\"\n",
    "    df = df.drop(columns=['filename'])\n",
    "    X_train_scaled, X_test_scaled_eg = preprocess_dataset(X_train, df)\n",
    "    return X_test_scaled_eg\n",
    "\n",
    "# for training the model\n",
    "simplified = pd.read_csv('simplified.csv')\n",
    "simplified['label'] = simplified['filename'].str.split('_').str[-2]\n",
    "simplified['label'].value_counts()\n",
    "\n",
    "X_train_simplified, y_train_simplified, X_test_simplified, y_test_simplified = split_dataset(simplified, 'filename', 0.30, 1)\n",
    "X_train_simplified = X_train_simplified.drop(columns=['label'])\n",
    "X_test_simplified = X_test_simplified.drop(columns=['label'])\n",
    "X_train_scaled_simplified, X_test_scaled_simplified = preprocess_dataset(X_train_simplified, X_test_simplified)\n",
    "# for training the model\n",
    "\n",
    "X_test_scaled_eg = preprocess(X_train_scaled_simplified, df)\n",
    "\n",
    "def train(model, X_train_scaled, y_train2, X_val_scaled, y_val2, batch_size):\n",
    "    \n",
    "    epochs = 100\n",
    "    times = []\n",
    "    train_dataloader, test_dataloader = intialise_loaders(X_train_scaled, y_train2, X_val_scaled, y_val2)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    tr_loss, tr_correct = [], []\n",
    "    te_loss, te_correct = [], []\n",
    "    for t in range(epochs):\n",
    "        train_loss, train_correct = train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "        test_loss, test_correct = test_loop(test_dataloader, model, loss_fn)\n",
    "\n",
    "        tr_loss.append(train_loss), tr_correct.append(train_correct)\n",
    "        te_loss.append(test_loss), te_correct.append(test_correct)\n",
    "        times.append(t+1)\n",
    "        \n",
    "        print(f\"Epoch {t+1}: Train_accuracy: {(100*train_correct):>0.2f}%, Train_loss: {train_loss:>8f}, Test_accuracy: {(100*test_correct):>0.2f}%, Test_loss: {test_loss:>8f}\")\n",
    "        \n",
    "    train_accuracies = tr_correct\n",
    "    train_losses = tr_loss\n",
    "    test_accuracies = te_correct\n",
    "    test_losses = te_loss\n",
    "\n",
    "    return train_accuracies, train_losses, test_accuracies, test_losses, times\n",
    "\n",
    "def intialise_loaders(X_train_scaled, y_train, X_test_scaled, y_test):\n",
    "\n",
    "    train_data = CustomDataset(X_train_scaled,y_train)\n",
    "    test_data = CustomDataset(X_test_scaled,y_test)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_data, batch_size=1024, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=1024, shuffle=True)\n",
    "    \n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    train_loss, train_correct = 0, 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    \n",
    "    train_loss /= num_batches\n",
    "    train_correct /=size\n",
    "\n",
    "    return train_loss, train_correct\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, test_correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            test_correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    test_correct /= size\n",
    "    \n",
    "    return test_loss, test_correct\n",
    "\n",
    "class FirstHiddenLayerMLP(nn.Module):\n",
    "\n",
    "    def __init__(self, no_features, no_hidden_first_layer, no_labels):\n",
    "        super().__init__()\n",
    "        self.mlp_stack = nn.Sequential(\n",
    "            nn.Linear(no_features, no_hidden_first_layer),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(no_hidden_first_layer, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(128, no_labels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "         logits = self.mlp_stack(x)\n",
    "         return logits\n",
    "\n",
    "model = FirstHiddenLayerMLP(77,256,2)\n",
    "train_accuracies, train_losses, test_accuracies, test_losses, times = train(model, X_train_scaled_simplified, y_train_simplified, X_test_scaled_simplified, y_test_simplified, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418482bd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "e49ee8a7-d9b2-499d-8394-d6cb86f4cb60",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b0c0b2a92c7d501f1ac652e11f948461",
     "grade": true,
     "grade_id": "cell-fbe8ba077fb74598",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91f8f29a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "b6756ab6-92e0-4a5e-b4b9-aebe009f5480",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "96d2e019d65c49ba15b3089c2184f021",
     "grade": false,
     "grade_id": "cell-48b4edbfec330f39",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "5. Do a model prediction on the sample test dataset and obtain the predicted label using a threshold of 0.5. The model used is the optimized pretrained model using the selected optimal batch size and optimal number of neurons. Note: Please define the variable of your final predicted label as 'pred_label'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fbc4115",
   "metadata": {
    "deletable": false,
    "id": "8fa3afdf-eed6-47b9-9acc-bc2304c46ec3",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "276ec9575db4ca701823a459809ea810",
     "grade": true,
     "grade_id": "cell-e83cb49660edc2b7",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_label is 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "X_test_scaled_eg = torch.tensor(X_test_scaled_eg, dtype=torch.float32)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test_scaled_eg)\n",
    "threshold = 0.5\n",
    "predicted_labels = (predictions > threshold).type(torch.long)\n",
    "\n",
    "predicted_label_array = predicted_labels.numpy()\n",
    "pred_label = np.argmax(predicted_label_array)\n",
    "print(\"pred_label is \" + str(pred_label))\n",
    "\n",
    "# The one hot vector has the value of 1 on the zeroth column.\n",
    "# Zeroth column is for negative class and first column is for positive class.\n",
    "# So record.wav has been classified as having negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667d83dd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "da2fc2cc-b89f-4fc3-af16-e30b4e5315a3",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "704df2be8fbd85ba163a89cd2e0431f0",
     "grade": true,
     "grade_id": "predict_value",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e45280ce",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "baab6e4d-4e8b-4358-a68d-682f60db4a06",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eac3438866b5ebd40f5fb20a676059bd",
     "grade": false,
     "grade_id": "cell-896f18b6b0b948ea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "6. Find the most important features on the model prediction for your test sample using SHAP. Create an instance of the DeepSHAP which is called DeepExplainer using traianing dataset: https://shap-lrjball.readthedocs.io/en/latest/generated/shap.DeepExplainer.html.\n",
    "\n",
    "Plot the local feature importance with a force plot and explain your observations.  (Refer to the documentation and these three useful references:\n",
    "https://christophm.github.io/interpretable-ml-book/shap.html#examples-5,\n",
    "https://towardsdatascience.com/deep-learning-model-interpretation-using-shap-a21786e91d16,  \n",
    "https://medium.com/mlearning-ai/shap-force-plots-for-classification-d30be430e195)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3872830",
   "metadata": {
    "deletable": false,
    "id": "081aa567-cd92-4749-93fd-fc6608a1f6ae",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db46b1b26fd45359768421987104ac3e",
     "grade": true,
     "grade_id": "importance_weight",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFit the explainer on a subset of the data (you can try all but then gets slower)\\nReturn approximate SHAP values for the model applied to the data given by X.\\nPlot the local feature importance with a force plot and explain your observations.\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Fit the explainer on a subset of the data (you can try all but then gets slower)\n",
    "Return approximate SHAP values for the model applied to the data given by X.\n",
    "Plot the local feature importance with a force plot and explain your observations.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
